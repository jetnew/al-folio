<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml"/><link href="/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-01-06T06:41:06+00:00</updated><id>/feed.xml</id><title type="html">blank</title><subtitle>Jet New&apos;s website. </subtitle><entry><title type="html">PCA Explained Simply</title><link href="/blog/2023/explain-pca/" rel="alternate" type="text/html" title="PCA Explained Simply"/><published>2023-01-09T00:00:00+00:00</published><updated>2023-01-09T00:00:00+00:00</updated><id>/blog/2023/explain-pca</id><content type="html" xml:base="/blog/2023/explain-pca/"><![CDATA[<p>Hi! If you are reading this, you are either:</p> <ul> <li>not a data scientist but want to understand why PCA should be used</li> <li>a data scientist but want to explain to a colleague why PCA should be used</li> </ul> <p>I recently realised that explaining Principal Component Analysis (PCA) to non-scientists is a universally common experience that all data scientists eventually do.</p> <p>I will explain when PCA is needed, what PCA does, an example with PCA, what benefits PCA brings, and how to interpret the results of PCA.</p> <p>I will <b>not</b> detail how PCA works, how to use PCA, or whether to use PCA or other methods, but I link nice resources in the FAQ.</p> <p>If you have questions not addressed in the FAQ, please leave a comment and I will answer you.</p> <hr/> <p><br/></p> <h2 id="why-is-pca-needed">Why is PCA needed?</h2> <p>PCA reduces the number of dimensions of the dataset.</p> <div class="row mt-3"> <div class="col-sm-2 mt-3 mt-md-0"></div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cat-shadow-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cat-shadow-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cat-shadow-1400.webp"/> <img src="/assets/img/cat-shadow.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-2 mt-3 mt-md-0"></div> </div> <div class="caption"> As an analogy, like how shadows are a 2D representation of 3D objects, PCA finds the best "angle" to cast the "shadow" of the dataset. Credits to <a href="https://unsplash.com/photos/JIuZZ0-EC0U">Evan Qu</a>. </div> <p>Thus, PCA is needed if you:</p> <ul> <li>have a very high-dimensional dataset (e.g. ≥100 features)</li> <li>cannot afford a large amount of compute (minutes vs hours vs days)</li> <li>want to make sense of many features (e.g. to select important features)</li> </ul> <p>By applying PCA, you:</p> <ul> <li>save time (during model training and model iteration)</li> <li>save space (in memory and model parameters)</li> <li>trade-off a little bit of performance</li> </ul> <hr/> <p><br/></p> <h2 id="what-does-pca-do">What does PCA do?</h2> <p>In a sentence:</p> <blockquote> PCA produces new features as a <u><b>weighted sum</b></u> of the original features in the dataset, by <u><b>preserving variance</b></u> in the original features. </blockquote> <p>What is a <b>weighted sum</b>?</p> \[X = w_1\cdot x_1 + \dotsb + w_n\cdot x_n\] <p>\(X\) is a sum of values \(x_1,\dotsc, x_n\), each weighted by \(w_1,\dotsc, w_n\) respectively. Likewise, PCA produces new features, e.g. \(X_1, X_2,\dotsc\), as weighted sums of original features \(x_1,\dotsc, x_n\).</p> <p>Why <b>preserve variance</b>?</p> <p>Preserving variance in the original features retains information about the spread of features in the data, usually resulting in good representations of original features for machine learning.</p> <hr/> <p><br/></p> <h2 id="show-me-an-example">Show me an example!</h2> <p>Let’s walk through PCA with the <a href="https://www.kaggle.com/c/boston-housing">Boston Housing dataset</a>:</p> <p>Given a dataset of 37 numerical features of houses, you want to predict its sale price. However, dealing with so many features can be (imagine) expensive in compute and difficult to interpret.</p> <details> <summary><small>Code</small></summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="err">!</span><span class="n">kaggle</span> <span class="n">competitions</span> <span class="n">download</span> <span class="o">-</span><span class="n">c</span> <span class="n">house</span><span class="o">-</span><span class="n">prices</span><span class="o">-</span><span class="n">advanced</span><span class="o">-</span><span class="n">regression</span><span class="o">-</span><span class="n">techniques</span>
<span class="err">!</span><span class="n">unzip</span> <span class="n">house</span><span class="o">-</span><span class="n">prices</span><span class="o">-</span><span class="n">advanced</span><span class="o">-</span><span class="n">regression</span><span class="o">-</span><span class="n">techniques</span><span class="p">.</span><span class="nb">zip</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">train.csv</span><span class="sh">'</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">train</span><span class="p">.</span><span class="nf">_get_numeric_data</span><span class="p">().</span><span class="nf">dropna</span><span class="p">()</span>
<span class="n">df</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span></code></pre></figure> </details> <table border="1" class="dataframe" style="width: 100%"> <thead> <tr style="text-align: right;"> <th></th> <th>Id</th> <th>MSSubClass</th> <th>LotFrontage</th> <th>LotArea</th> <th>OverallQual</th> <th>OverallCond</th> <th>YearBuilt</th> <th>YearRemodAdd</th> <th>MasVnrArea</th> <th>...</th> <th>SalePrice</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>1</td> <td>60</td> <td>65.0</td> <td>8450</td> <td>7</td> <td>5</td> <td>2003</td> <td>2003</td> <td>196.0</td> <td>...</td> <td>208500</td> </tr> <tr> <th>1</th> <td>2</td> <td>20</td> <td>80.0</td> <td>9600</td> <td>6</td> <td>8</td> <td>1976</td> <td>1976</td> <td>0.0</td> <td>...</td> <td>181500</td> </tr> <tr> <th>2</th> <td>3</td> <td>60</td> <td>68.0</td> <td>11250</td> <td>7</td> <td>5</td> <td>2001</td> <td>2002</td> <td>162.0</td> <td>...</td> <td>223500</td> </tr> <tr> <th>3</th> <td>4</td> <td>70</td> <td>60.0</td> <td>9550</td> <td>7</td> <td>5</td> <td>1915</td> <td>1970</td> <td>0.0</td> <td>...</td> <td>140000</td> </tr> <tr> <th>4</th> <td>5</td> <td>60</td> <td>84.0</td> <td>14260</td> <td>8</td> <td>5</td> <td>2000</td> <td>2000</td> <td>350.0</td> <td>...</td> <td>250000</td> </tr> </tbody> </table> <p>5 rows x 38 columns</p> <p>Because PCA reduces the number of dimensions of the dataset, model training can take a shorter time without sacrificing too much performance.</p> <p>Let’s run PCA on the 37 features:</p> <details> <summary><small>Code</small></summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="sh">'</span><span class="s">SalePrice</span><span class="sh">'</span><span class="p">).</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">SalePrice</span><span class="sh">'</span><span class="p">])</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">pca</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_features</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_features_test</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Original features (normalized):</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">round</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">PCA features:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">X_features</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">round</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span></code></pre></figure> </details> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Original features (normalized):
 [-0.646  0.136  0.171 -0.252  1.297 -0.523  1.221  1.129  0.026  0.221
 -0.283 -0.675 -0.531 -0.856  1.734 -0.106  0.784  1.175 -0.233  0.79
  1.238  0.175 -0.202  0.9   -0.943  1.214  0.211  0.271 -0.746  1.613
 -0.363 -0.114 -0.285 -0.074 -0.146  2.113  0.879]

PCA features:
 [ 2.141 -1.273]
</code></pre></div></div> <hr/> <p><br/></p> <h2 id="what-benefits-does-pca-bring-at-what-trade-off">What benefits does PCA bring? At what trade-off?</h2> <p>Let’s train 2 linear regression models:</p> <ol> <li>Trained on the original 37-dimensional features</li> <li>Trained on the 2-dimensional features produced by PCA</li> </ol> <h4 id="performance-a-tiny-trade-off">Performance: A Tiny Trade-off</h4> <p>Even though PCA reduces the number of dimensions from 37 to merely 2, there is a small decrease in test error of only <b>-0.02</b> (RMSE of log of sale price as evaluated on Kaggle).</p> <details> <summary><small>Code</small></summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="n">lr</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_train_hat</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Original Features (37 dimensions)</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Train error:</span><span class="sh">"</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_hat</span><span class="p">)).</span><span class="nf">round</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span> 
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Test error:</span><span class="sh">"</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)).</span><span class="nf">round</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>

<span class="n">lr_pca</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span>
<span class="n">lr_pca</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_features</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_train_hat</span> <span class="o">=</span> <span class="n">lr_pca</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_features</span><span class="p">)</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">lr_pca</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_features_test</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">PCA features (2 dimensions)</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Train error:</span><span class="sh">"</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_hat</span><span class="p">)).</span><span class="nf">round</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span> 
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Test error:</span><span class="sh">"</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)).</span><span class="nf">round</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span></code></pre></figure> </details> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Original Features (37 dimensions)
Train error: 0.15
Test error: 0.1348

PCA features (2 dimensions)
Train error: 0.1935
Test error: 0.1562
</code></pre></div></div> <h4 id="training-time-a-major-speed-up">Training Time: A Major Speed-up</h4> <p>With PCA, because number of features is drastically reduced (from 37 to 2), training time taken is also reduced from 4.85ms to 584µs, a <b>8300X</b> speedup!</p> <details> <summary><small>Code (Linear regression with original features)</small></summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">timeit</span>

<span class="n">lr</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_train_hat</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span></code></pre></figure> </details> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>4.85 ms ± 276 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre></div></div> <details> <summary><small>Code (Linear regression with PCA features)</small></summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">timeit</span>

<span class="n">lr_pca</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span>
<span class="n">lr_pca</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_features</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_train_hat</span> <span class="o">=</span> <span class="n">lr_pca</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_features</span><span class="p">)</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">lr_pca</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_features_test</span><span class="p">)</span></code></pre></figure> </details> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>584 µs ± 19 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
</code></pre></div></div> <h4 id="memory-space-a-significant-reduction">Memory Space: A Significant Reduction</h4> <p>Because the no. of dimensions decreased by 18.5X, so did memory space used, <b>from 265Kb to 14Kb</b>.</p> <details> <summary><small>Code</small></summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Memory size of data w/o PCA: </span><span class="si">{</span><span class="n">X_train</span><span class="p">.</span><span class="n">size</span> <span class="o">*</span> <span class="n">X_train</span><span class="p">.</span><span class="n">itemsize</span><span class="si">}</span><span class="s"> bytes</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Memory size of data w/ PCA: </span><span class="si">{</span><span class="n">X_features</span><span class="p">.</span><span class="n">size</span> <span class="o">*</span> <span class="n">X_features</span><span class="p">.</span><span class="n">itemsize</span><span class="si">}</span><span class="s"> bytes</span><span class="sh">"</span><span class="p">)</span></code></pre></figure> </details> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Memory size of data w/o PCA: 265216 bytes
Memory size of data w/ PCA: 14336 bytes
</code></pre></div></div> <hr/> <p><br/></p> <h2 id="how-to-interpret-pca-results">How to interpret PCA results</h2> <p>Recall what PCA does:</p> <blockquote>PCA produces new features as a <u><b>weighted sum</b></u> of original features, by <u><b>preserving variance</b></u> in the original features.</blockquote> <p>PCA produces new features, \(X_1, X_2\) as a weighted sum of the original features, \(x_1, x_2, x_3\).</p> <p>In other words, we can get \(X_1\) and \(X_2\) using \(PC_1\) and \(PC_2\), by computing a weighted sum of \([x_1, x_2, x_3]\).</p> \[X_1 = w_{11}x_1 + w_{12}x_2 + w_{13}x_3\] \[X_2 = w_{21}x_1 + w_{22}x_2 + w_{23}x_3\] <p>Compare weighted sums with PCA features:</p> <details> <summary><small>Code</small></summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">PC1 weighted sum:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">sum</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">components_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="nf">round</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">PC2 weighted sum:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">sum</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">components_</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="nf">round</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">PCA features:</span><span class="sh">"</span><span class="p">,</span> <span class="n">X_features</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">round</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span></code></pre></figure> </details> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PC1 weighted sum: 1.515
PC2 weighted sum: 0.968
PCA features: [1.515 0.968]
</code></pre></div></div> <p>Inspecting principal component 1, the highest weighted feature is <code class="language-plaintext highlighter-rouge">OverallQual</code>. This means that <code class="language-plaintext highlighter-rouge">OverallQual</code> has the highest variance of all features in the dataset, and is likely a useful feature for sale price prediction.</p> <p>Sort features by the 1st principal component’s weights:</p> <details> <summary><small>Code</small></summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="nf">use</span><span class="p">(</span><span class="sh">'</span><span class="s">seaborn</span><span class="sh">'</span><span class="p">)</span>

<span class="n">i</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">components_</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="nf">argsort</span><span class="p">()</span>
<span class="n">pca_component_1</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="n">components_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
<span class="n">columns</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="sh">'</span><span class="s">SalePrice</span><span class="sh">'</span><span class="p">).</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">barh</span><span class="p">(</span><span class="n">columns</span><span class="p">,</span> <span class="n">pca_component_1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span></code></pre></figure> </details> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pca-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pca-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pca-1400.webp"/> <img src="/assets/img/pca.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <p><br/></p> <h2 id="faq">FAQ</h2> <details><summary> How does PCA preserve variance?</summary> WIP </details> <details><summary>When should I not use PCA?</summary> WIP </details> <details><summary>What are principal components?</summary> $$ \begin{align} \begin{bmatrix}X_1\\X_2\end{bmatrix} =&amp; \begin{bmatrix}PC_1\\PC_2\end{bmatrix} \begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}\\ =&amp; \begin{bmatrix} w_{11} &amp; w_{12} &amp; w_{13}\\ w_{21} &amp; w_{22} &amp; w_{23} \end{bmatrix} \begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}\\ =&amp; \begin{bmatrix} w_{11}x_1 + w_{12}x_2 + w_{13}x_3\\ w_{21}x_1 + w_{22}x_2 + w_{23}x_3 \end{bmatrix} \end{align} $$ </details>]]></content><author><name></name></author><category term="tutorial"/><category term="data-science"/><summary type="html"><![CDATA[A simple explainer of why PCA is needed and how to interpret it.]]></summary></entry><entry><title type="html">Industry vs PhD</title><link href="/blog/2022/industry-vs-phd/" rel="alternate" type="text/html" title="Industry vs PhD"/><published>2022-10-12T00:00:00+00:00</published><updated>2022-10-12T00:00:00+00:00</updated><id>/blog/2022/industry-vs-phd</id><content type="html" xml:base="/blog/2022/industry-vs-phd/"><![CDATA[<h2 id="background">Background</h2> <p>Over the past 2 years, I have deliberated too long over my career plans after graduation, in particular, to work in industry or to pursue a PhD to work in research. I have finally decided to pursue a data scientist position with Indeed, instead of proceeding with my plans to apply to UCL and Edinburgh to research on reinforcement learning.</p> <p>I list all factors I considered that may be unique to my personal situation, then later provide a more general framework for other students in tech to consider between industry and PhD. This list first serves as a personal track to later reflect on, and then to compile a list of considerations to help other students with their decision making.</p> <p>My possibilities after university are:</p> <ol> <li>Industry - Data Scientist <ul> <li>Indeed Singapore, Match Recommendation Platform</li> </ul> </li> <li>(Applying to) PhD - AI Researcher <ul> <li>University College London, Deciding, Acting and Reasoning with Knowledge (DARK) Lab</li> <li>University of Edinburgh, Autonomous Agents Research Group</li> </ul> </li> </ol> <h2 id="personal-factors-of-consideration">Personal Factors of Consideration</h2> <p><strong>Indeed: Pros</strong></p> <p>Salary</p> <ul> <li>Competitive tech pay</li> <li>Provides masters sponsorship</li> <li>Higher quality of life because of pay</li> <li>Indeed’s business is doing well, important as global recession is coming</li> <li>Help raise financial security for mother and sister, significantly reducing stress for my mother</li> </ul> <p>Work Life Balance</p> <ul> <li>Excellent work life balance</li> <li>Free time to focus on other aspects of life (family, relationships, hobbies, goals)</li> <li>Great people culture and nice personalities</li> <li>Workplace is convenient to meet my sister and my girlfriend</li> <li>Less stress thanks to work-life balance</li> <li>Possible to work on side projects</li> </ul> <p>Career progression</p> <ul> <li>Match Recommendation Platform team - metrics and achievements have immense technical and business impact</li> <li>Competitive, fair and supportive career progression and compensation</li> <li>One of very few big tech companies to hire fresh grads as data scientists</li> </ul> <p>Industry relevance</p> <ul> <li>Already proven myself as a competent data scientist as a fresh grad</li> <li>Industry skills and experiences are useful in any company</li> <li>Learn engineering, business and company organization which Indeed has done well</li> <li>Leverage my strengths in leadership, product dev, data science and engineering that are valued in industry</li> <li>Possibility of publishing at RecSys conferences</li> </ul> <p>Business impact</p> <ul> <li>Can attend industry events - PyCon</li> <li>Fly to US/Japan offices for work</li> <li>Significant flexibility to work on my ideas if I want to</li> </ul> <p><strong>Indeed: Cons</strong></p> <ul> <li>Sometimes work may be less interesting (admin, dry engineering work)</li> <li>Work in Singapore is stifling and need to squeeze in the MRT</li> <li>Data science is no longer perceived to be as prestigious as AI research</li> </ul> <p><strong>PhD: Pros</strong></p> <p>Passion</p> <ul> <li>Fun work on my interest in reinforcement learning and games</li> <li>AI research is cool</li> <li>Research methodology teaches rigorous analysis and experimentation</li> </ul> <p>Social recognition</p> <ul> <li>PhD or Dr title</li> <li>Getting the prestigious university brand</li> <li>Generally perceived as being not dumb</li> <li>Recognition of doing well in studies</li> <li>Can work with the smartest scientists to improve the world</li> </ul> <p>Living abroad</p> <ul> <li>Work and live in the US/UK overseas</li> </ul> <p>Industry research</p> <ul> <li>Conventional way of being an AI researcher</li> <li>Path into AI research in FAANG</li> </ul> <p><strong>PhD: Cons</strong></p> <p>Financial</p> <ul> <li>$3K to $5K stipend is low</li> <li>High cost of living in US/UK</li> <li>Opportunity cost of $200K over term of PhD (estimated cost of raising a child in Singapore till age 21)</li> </ul> <p>Lifestyle</p> <ul> <li>Age 30 when I graduate PhD</li> <li>Postpone or difficult life plans (marriage, housing, long-distance relationship)</li> <li>Need to re-learn to live a new life with new friends in a new environment</li> <li>Being a foreigner in a foreign country subjects myself to potential discrimination</li> </ul> <p>Risk</p> <ul> <li>No safety net if wrong or bad PhD advisor</li> <li>Risky as publication record may not be good enough to get a FAANG researcher role after PhD</li> <li>Bet on RL becoming industry-ready and impactful in 5 years may not come true</li> <li>Success in research is very largely a function of pure intellectual horsepower, while my strength is a diversity of competencies</li> </ul> <p>Industry research</p> <ul> <li>Begin career progression only after graduating from PhD as a junior researcher</li> <li>Need to prove myself to be a capable researcher in industry</li> <li>PhD is largely powered by prestige - which university, which labs, which advisors and which people you know largely determines your research career trajectory</li> </ul> <h2 id="questions-for-others-to-consider">Questions for Others to Consider</h2> <p><strong>Industry</strong></p> <p>Salary</p> <ul> <li>What salary is suffient for you to feel secure and comfortable?</li> <li>What salary is reflective of the competitiveness of your skills?</li> </ul> <p>Work-life balance</p> <ul> <li>Are there other aspects of life that are equally if not more important to you?</li> <li>Is having free leisure time valuable to you?</li> </ul> <p>Career progression</p> <ul> <li>What is your “end game” in the peak of your career?</li> <li>How significant is the financial opportunity cost to your personal situation?</li> </ul> <p>Engineering skills</p> <ul> <li>How valuable are engineering skills for achieving your career goals?</li> <li>Are your natural strengths in engineering?</li> </ul> <p>Business Impact</p> <ul> <li>How important is industry relevance to you in your professional career?</li> <li>Is impact a significant metric to you in a professional career?</li> </ul> <p><strong>PhD</strong></p> <p>Passion</p> <ul> <li>How important is it to you to work on what you love?</li> <li>Can you foresee yourself working in single research topic for the next 5 years?</li> </ul> <p>Social recognition</p> <ul> <li>How important is prestige and status to your personal social well-being?</li> <li>How important is it to be able to collaborate with other brilliant scientists?</li> </ul> <p>Living abroad</p> <ul> <li>How socially and financially comfortable are you about living abroad?</li> </ul> <p>Research impact</p> <ul> <li>How important is making an impactful contribution to the progress of science to you?</li> <li>How confident are you to potentially make a significant intellectual contribution to science?</li> </ul> <p>Industry research</p> <ul> <li>Must you work in an industry research lab after graduating from PhD?</li> </ul>]]></content><author><name></name></author><category term="life"/><category term="research"/><summary type="html"><![CDATA[A list of personal factors of consideration to look back on years later.]]></summary></entry><entry><title type="html">100 Lessons from 1 Year of AI Research</title><link href="/blog/2021/100-lessons/" rel="alternate" type="text/html" title="100 Lessons from 1 Year of AI Research"/><published>2021-12-01T00:00:00+00:00</published><updated>2021-12-01T00:00:00+00:00</updated><id>/blog/2021/100-lessons</id><content type="html" xml:base="/blog/2021/100-lessons/"><![CDATA[<h2 id="about-these-lessons">About these Lessons</h2> <p>I’m an undergraduate from the National University of Singapore. I researched reinforcement learning for a full year, including full-time over winter and summer breaks, as part of an undergraduate research <a href="https://www.comp.nus.edu.sg/programmes/ug/project/urop/">programme</a>. I faced many uncertainties and difficulties while doing research over the year, and deliberated over choosing to spending time on research versus on industry-relevant skills.</p> <p>This blog post condenses all the hard-earned lessons I’ve learnt after stumbling through mistakes and picking myself up again. After 1 year of research, I believe that my decision to spend 1 year’s worth of time on research was well worth it, purely from my takeaways, even though I ended up without a publication that I desired so much at the start of my journey. In writing this post, I hope that fellow researchers that face related challenges about research can learn without going through the hard way, and I believe that this post will be helpful to refer back to when stuck on various difficulties related to research.</p> <p>This post is structured by lessons on:</p> <ol> <li>Research - <em>Insights about research methodology</em></li> <li>Learning - <em>Reflections about learning effectively</em></li> <li>Reinforcement Learning - <em>Difficulties I faced with my research topic</em></li> <li>Workflow - <em>Hard-earned lessons that incurred significant costs</em></li> <li>Motivation - <em>Introspections about how to advance myself sustainably</em></li> <li>Support - <em>Acknowledgements for the social structures that enabled me</em></li> <li>Mindset - <em>Hard-earned lessons about life that I learnt from research</em></li> </ol> <p>Each lesson consists of an advice, a brief explanation and the lesson through which I learnt it.</p> <p>I would like to thank my research advisor <a href="https://haroldsoh.github.io/">Prof. Harold Soh</a> from the <a href="https://clear-nus.github.io/">Collaborative, Learning and Adaptive Robots (CLeAR) Lab</a> at NUS for his mentorship, my seniors in the lab Bingcai and Kaiqi for their advice, my close friends Shiying, Jun Jie, Wai Ching, Ming Liang and Liying for their support, and many other people who have influenced and helped me in this short but memorable 1-year journey.</p> <p>I am also looking for a research internship in 2022, ideally reinforcement learning; please <a href="mailto:notesjet@gmail.com">let me know</a> if you are recruiting!</p> <h2 id="research">Research</h2> <ol> <li>The problem statement is the most important thing in research because without one, there’s no real need for the research idea.</li> <li>Develop a long-term vision of your research direction to avoid tunnel-visioning into a few specific lines of research.</li> <li>Reflect on the significance of problems that papers aim to address as there exists papers that explore ideas without addressing a real problem.</li> <li>When reading papers, reflect about the magnitude of improvement made towards addressing the targeted problem.</li> <li>Understand the significance of performance improvements with respect to the problem, such as the units or measure used.</li> <li>When reading papers, analyse closely how the authors overcame difficulties to make the idea work, because papers tend to highlight successes, unintentionally downplaying the difficulties needed to be overcome to make the proposed idea work.</li> <li>When evaluating ideas in papers, consider the compute and space required, and whether you can afford to try those ideas given your constraints.</li> <li>Simple, elegant ideas are preferable to complex, “Frankenstein” ideas that can seem more like a series of hacks.</li> <li>Try to work towards significant innovations instead of “<em>delta improvements</em>” that generates only little or even negligible insights.</li> <li>Research experiments should be minimally sufficient and target the hypothesis well so that time is not wasted due to redundant results.</li> <li>Research papers are about developing insights, not chasing performance improvements, since significant insights will lead to many subsequent papers.</li> <li>During idea generation, focus on how you’d solve problems and <em>less</em> on others’ existing solutions.</li> <li>Evaluate problem significance not just on small “toy problems”, as problems could turn out massive when scaled up or completely vanish in a real-world setting.</li> <li>Reflect on difficulties in your research field even if they may not be directly related to your targeted problem as those may eventually hinder your progress if left neglected.</li> <li>Don’t just read or you’ll never get started. At some point, start formulating potential solutions to problems.</li> <li>Consider the time requirements when scoping your problem, idea or experiment, as working on them may turn out infeasible with limited resources.</li> <li>Invest time in learning how to do research and not just on your research topic.</li> <li>Choose a research topic that your lab specialises in, not one that is indirectly related, or choose a lab according to your research interests.</li> <li>A research idea that is elegant is one that is minimally sufficient.</li> <li>Avoid being overambitious with a research idea, otherwise, chances are that the method won’t work or it works but you don’t understand why.</li> <li>Always situate your problem, motivation and experiment design around existing literature to enable comparison and context to justify design choices.</li> <li>Don’t take it personally if an idea that seems justified initially turns out ineffective, since not all ideas work well, yet some simple ideas can be unreasonably effective.</li> </ol> <h2 id="learning">Learning</h2> <ol> <li>Ensure a strong mastery of foundations.</li> <li>Don’t shy away from difficult topics — break them down into more manageable components, focusing on the years-long goal of mastery.</li> <li>Keep updated with research papers published across multiple different conferences.</li> <li>Frequently consolidate knowledge you learnt, through summaries and mindmaps.</li> <li>After reading many papers, write a survey to organise the structure of existing literature and synthesise current research problems, methods and directions.</li> <li>Especially important for complex topics, create a mindmap for the large diversity of existing problems, tasks and approaches in your field of research for an overview perspective.</li> <li>Summarise papers and their key contributions to enable efficient revision and future reference.</li> <li>Teach and present paper summaries to others to ensure you understand the nuances of the paper and get other perspectives that you might not have considered.</li> <li>Invest time in learning topics in adjacent or relevant fields, even if they may not be immediately related.</li> <li>Before attempting to generate potential paper ideas, ensure that you have a robust understanding of the current literature, otherwise time may be wasted when another paper might have already hinted at its ineffectiveness.</li> <li>Remember that online resources for explanations and code walkthroughs are available.</li> <li>Before formulating ideas, dive into the theory to thoroughly understand the nuances between the theory of different works.</li> <li>Invest time in software development skills.</li> <li>Explore and work on ideas from adjacent fields; it usually brings new insights into your research.</li> <li>Don’t shy away from theory papers that seem difficult initially; they are a treasure trove of learning opportunities.</li> <li>Be daring to express opinions and ask questions that you’re uncertain about, so as to receive feedback that directly updates your views.</li> <li>Keep updated with “key highlight” papers in adjacent fields, as such influential ideas may eventually reach into your research topic.</li> </ol> <h2 id="reinforcement-learning">Reinforcement Learning</h2> <ol> <li>Remember to consider variance of results and the usage of random seeds.</li> <li>Algorithms used for baseline comparisons should be evaluated with proper hyperparameter tuning.</li> <li>Consider the large number of possible hyperparameter configurations in experiment formulation.</li> <li>List all possible modes of failure after every experiment so that you can diagnose later problems easier.</li> <li>Bugs in the code are more likely to be the cause of unexpected inferior performance rather than the idea itself.</li> <li>Scope your research project according to available resources, such as focusing on problems that fundamentally require less compute, e.g. Data-Efficienct RL.</li> <li>Inspect papers’ design choices thoroughly, as other options are likely to be less ideal.</li> <li>Scope your project to focus on and analyse only a few selected components necessary to target the problem of interest, especially if it has many interacting components, e.g. in model-based reinforcement learning.</li> <li>Be aware of research challenges present in your research field, e.g. reproducibility and benchmarking.</li> </ol> <h2 id="workflow">Workflow</h2> <ol> <li>Justify experiment steps with the hypothesis it aims to answer, so that you don’t waste time on experiments that don’t generate insights.</li> <li>Begin with simpler or smaller experiments to validate problems and ideas before moving to more complex and bigger ones.</li> <li>Remember to parallelise experiments, even using CPUs alongside GPUs can be very useful.</li> <li>Always consider all computational resources that are available to you and utilise them to the fullest, such as cloud compute credits.</li> <li>Use a dedicated reference manager while reading papers, e.g. Zotero or Mendeley.</li> <li>Invest time in setting up proper experimental tools and configurations, e.g. logging, Weights &amp; Biases, code refactoring, as the time cost pays off much earlier than you expect.</li> <li>Categorise the papers you’ve read properly, such as through tags using a reference manager.</li> <li>Track dependencies and environment configurations of your project’s code.</li> <li>Invest time in learning to use Linux and set up servers.</li> <li>Organize your work environment, be it your physical desk and research papers, your digital notes and PDFs, or your saved Tweets and bookmarks.</li> <li>Don’t necessarily work and update your advisor week after week, since ideas sometimes require more than 1 week to develop before the justification seems sound.</li> <li>Make it a habit to work on research a few hours a day, even if you seem to make no progress.</li> <li>Employ a systematic approach to reading papers, such as annotating or writing summaries.</li> <li>Set deadlines on each individual task and keep to them strictly.</li> <li>Narrow down and formulate your research project exactly, so as to better structure your research methodology in the short term.</li> <li>Remember to clear space from trash folders.</li> <li>If you can, work on only a few commitments at a time, so that you have free time and space to digest, synthesise and be creative with your research.</li> <li>Invest time in improving your workflow across all levels of work, e.g. note-taking frameworks, work-break sprints, or simply clearing your desk at the end of each day.</li> <li>Make sure that you understand everything before extending an existing code base, as one modification can affect another component unexpected, or even worse, fail silently.</li> <li>Commit a fixed ratio of time to explore topics adjacent to your research field.</li> </ol> <h2 id="support">Support</h2> <ol> <li>A community makes the journey of research easier.</li> <li>Be daring to reach out for help from seniors and more experienced people, even if it may seem trivial.</li> <li>Reach out to researchers to ask questions or request for help about the papers they author.</li> <li>Reach out often to fellow researchers to discuss ideas, especially those whose work you admire.</li> <li>Reach out to professors in your university; don’t wait for a dedicated research programme to get started with research.</li> <li>Research is not the only thing that matters in life; remember to take a break once in a while to spend time with family and friends.</li> <li>Try to bounce ideas off other people in your lab to get their perspectives.</li> <li>Get in touch with other researchers working in the same field as you, such as other professors in your university.</li> <li>Keep frequent contact and maintain a good relationship with your research advisor.</li> </ol> <h2 id="motivation">Motivation</h2> <ol> <li>Remember to review and acknowledge the progress you’ve made and successes you’ve achieved.</li> <li>Celebrate small wins consistently.</li> <li>Use paper submission deadlines as a goal to work towards and be motivated about.</li> <li>Remember that the goal of publishing should only be used for motivation and not your main objective of research.</li> <li>Don’t fall prey to personal comparisons with other researchers, such as differences in competence, starting points, resources or available guidance, because everyone’s journey is unique.</li> <li>Remember to maintain your health and fitness.</li> <li>Know what motivates you and learn to manage your motivation.</li> </ol> <h2 id="mindset">Mindset</h2> <ol> <li>Great things take time.</li> <li>It’s okay not to know many things; you only need to know a few things deeply.</li> <li>Don’t let yourself be too affected by the opportunity costs of doing research.</li> <li>Be persistent on hammering at the problem and don’t give up until you’ve exhausted all possible modes of failure, because research problems <em>are</em> difficult.</li> <li>It’s alright to not achieve concrete achievements such as publications, especially in your first few years of research.</li> <li>Stop thinking about the past and less-ideal starting points that you possibly had, and focus on what you can do now to improve yourself from here onwards.</li> <li>Be humbled by the difficulty and complexity of research; don’t be overconfident about your abilities.</li> <li>Know your strengths, embrace your weaknesses, and then leverage your strengths as much as possible.</li> <li>Research has a long-horizon and sparse-reward credit assignment problem, as results may not show until months or even years later.</li> <li>Don’t work on ideas simply because you think they might work. Instead, work on ideas that you are passionate about and it will show in your work.</li> <li>Sometimes, your best is not enough. And that’s alright.</li> <li>Focus on doing your best work, not a work that is better than others.</li> <li>Research is but one aspect of life.</li> <li>Your best idea is always your next idea, so if your current idea didn’t turn out well, look forward to your next idea and move on.</li> <li>Instead of focusing on failure, focus on the lessons you’d learnt in the process.</li> <li>You never really stop learning lessons.</li> </ol>]]></content><author><name></name></author><category term="life"/><category term="research"/><summary type="html"><![CDATA[Lessons from my Undergraduate Research Opportunities Programme at NUS.]]></summary></entry><entry><title type="html">Getting Started with Reinforcement Learning</title><link href="/blog/2021/rl-resources/" rel="alternate" type="text/html" title="Getting Started with Reinforcement Learning"/><published>2021-07-07T00:00:00+00:00</published><updated>2021-07-07T00:00:00+00:00</updated><id>/blog/2021/rl-resources</id><content type="html" xml:base="/blog/2021/rl-resources/"><![CDATA[<h2 id="what-is-reinforcement-learning-rl">What is Reinforcement Learning (RL)?</h2> <p>You may have heard about the successes of artificial intelligence (AI) over humans in games such as Go (<a href="https://www.bbc.com/news/technology-50573071">Go master quits because AI ‘cannot be defeated’</a>), StarCraft (<a href="https://www.theverge.com/2019/10/30/20939147/deepmind-google-alphastar-starcraft-2-research-grandmaster-level">DeepMind’s StarCraft 2 AI is now better than 99.8 percent of all human players</a>) and DoTA 2 (<a href="https://www.theverge.com/2019/4/13/18309459/openai-five-dota-2-finals-ai-bot-competition-og-e-sports-the-international-champion">OpenAI’s Dota 2 AI steamrolls world champion e-sports team with back-to-back victories</a>).</p> <p>The core technology behind these AI successes is reinforcement learning. Reinforcement learning is a <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> paradigm (alongside supervised and unsupervised learning) that learns an optimal sequence of actions to take in the environment. In contrast, <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a> learns a single output prediction based on an existing training dataset of labels, and <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised learning</a> learns the distribution of data without any given labels.</p> <p>As you may have expected, reinforcement learning is closely related to (and is in fact inspired by) the same concept behind <a href="https://en.wikipedia.org/wiki/Classical_conditioning">Pavlov’s Dog</a>, operant conditioning. <a href="https://en.wikipedia.org/wiki/Operant_conditioning">Operant conditioning</a> is a learning process that associates and modifies behaviours with reinforcement or punishment. This is well described by psychologist <a href="https://en.wikipedia.org/wiki/Edward_Thorndike">Edward Thorndike</a>’s <a href="https://en.wikipedia.org/wiki/Law_of_effect">Law of Effect</a>, which states:</p> <blockquote> <p>“Responses that produce a satisfying effect in a particular situation become more likely to occur again in that situation, and responses that produce a discomforting effect become less likely to occur again in that situation.”</p> </blockquote> <p>The process of reinforcement learning takes place in 4 stages:</p> <ol> <li>The environment’s state is first observed by the agent.</li> <li>The agent takes an action based on the observation.</li> <li>The agent’s action influences the environment’s state.</li> <li>The environment then returns a new state to the agent, along with a reward indicating the goodness of the action taken.</li> </ol> <p>Reinforcement learning is currently used by a few applications in industry, namely <a href="https://venturebeat.com/2021/02/23/how-reinforcement-learning-chooses-the-ads-you-see/">recommending personalised advertisements</a>, improving search engines, and more recently, <a href="https://engineering.grab.com/understanding-supply-demand-ride-hailing-data">improving pricing algorithms</a> of ride-hailing applications based on live demand and supply. While RL currently does not have as many applications as domains like <a href="https://en.wikipedia.org/wiki/Computer_vision">computer vision</a> and <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a>, RL is a promising and upcoming method with high potential in fields like <a href="https://bair.berkeley.edu/blog/2020/04/27/ingredients/">robotics</a> (think automating thousands of manual processes in factories), self-driving vehicles (think reducing accident rates while significantly improving logistics transportation efficiency) and, of course, video games (who doesn’t enjoy seeing games being solved/mastered?)</p> <p>Reinforcement learning can seem quite intimidating to beginners due to the vastness of the field. In this blog article, I will share the resources and some advice on how to use them effectively to get started with learning reinforcement learning. These recommendations are heavily opinionated based on my own learning journey in RL.</p> <hr/> <h2 id="introductory-reinforcement-learning">Introductory Reinforcement Learning</h2> <p>Reinforcement learning is a sub-field of machine learning, and so learning RL will require learning machine learning. You should also learn RL in consideration of mathematical theory and practical coding. The following 3 resources are thus recommended based on those considerations.</p> <h3 id="1-machine-learning-crash-course-by-google">1. <a href="https://developers.google.com/machine-learning/crash-course/ml-intro">Machine Learning Crash Course</a> by Google</h3> <p>Reinforcement learning requires many concepts from machine learning (ML). This crash course (that contains mainly explanatory videos) provides an introductory understanding to the main ML concepts that you will need for RL. Some concepts covered: Training vs Validation vs Testing, Classification vs Regression, Introductory Neural Networks.</p> <h3 id="2-reinforcement-learning-a-tutorial-by-harmon--harmon">2. <a href="http://www.cs.toronto.edu/~zemel/documents/411/rltutorial.pdf">Reinforcement Learning: A Tutorial</a> by Harmon &amp; Harmon</h3> <p>This tutorial presents the theory of RL at an introductory level. It formalises the RL paradigm with math, and is a great resource so that concepts are rigorous and not too hand-wavy. The math and notation may make it look intimidating, but if you understand the underlying motivation and concepts, understanding the notation will come naturally. Follow through thoroughly and you will be rewarded well. Topics covered: Dynamic programming, Value iteration, Q-learning, Temporal difference learning, Discounting rewards. Nonetheless, even if you’re unable to follow through, you should still face no problem proceeding with the next resource on Medium.</p> <h3 id="3-deep-rl-course-on-medium-by-thomas-simonini">3. <a href="https://simoninithomas.github.io/deep-rl-course/">Deep RL Course on Medium</a> by Thomas Simonini</h3> <p>This Medium series on deep RL (deep because it uses deep neural networks) is one of the best courses for learning introductory deep RL with a good balance of practical coding and RL theory. Topics covered: Deep Q-learning, Policy Gradients, Actor Critics, Unity-ML Agents. The resources up to actor critics are excellent; Learning Unity-ML Agents is optional, learn it if you are interested in the Unity-ML Agents environments.</p> <hr/> <h2 id="tools-for-reinforcement-learning">Tools for Reinforcement Learning</h2> <h3 id="1-rl-gym-environments">1. RL Gym Environments</h3> <p>RL Gym environments are the simulation software that is used to train RL agents in. They’re called “gyms” because of <a href="https://gym.openai.com/">OpenAI gym</a>’s standardisation of environment APIs. There are too many RL environments, but here are some lists to refer to: <a href="https://github.com/clvrai/awesome-rl-envs">awesome-rl-envs</a>, <a href="https://github.com/tshrjn/env-zoo">env-zoo</a> and <a href="https://github.com/kengz/awesome-deep-rl">awesome-deep-rl</a>. The RL environments are diverse and many are really cool, such as IKEA furniture assembly, Retro games and autonomous vehicle simulators.</p> <h3 id="2-rl-frameworks">2. RL Frameworks</h3> <p>RL frameworks are frameworks made specifically for building RL agents and/or training them. While most RL research work use general deep learning frameworks, e.g. <a href="https://pytorch.org/">PyTorch</a>, <a href="https://www.tensorflow.org/">TensorFlow</a>, <a href="https://jax.readthedocs.io/en/latest/index.html">JAX</a>, <a href="https://github.com/google/flax">FLAX</a>, <a href="https://www.pytorchlightning.ai/">Pytorch Lightning</a>, <a href="https://www.tensorflow.org/probability">TensorFlow Probability</a>, there are also RL frameworks available. There is currently no established “best” RL frameworks yet due to the diversity of RL concepts, but I found <a href="https://stable-baselines3.readthedocs.io/en/master/">Stable Baselines</a> (recently version 3) very useful for basic usage of RL. <a href="https://www.tensorflow.org/agents">TensorFlow Agents</a> is a library used by some companies in industry. <a href="https://github.com/seungeunrho/minimalRL">minimalRL</a> and <a href="https://docs.cleanrl.dev/">CleanRL</a> are repositories of minimal code implementations of RL algorithms in PyTorch that is useful for learning.</p> <hr/> <h2 id="intermediate-reinforcement-learning">Intermediate Reinforcement Learning</h2> <p>Reinforcement learning is a vast field that contains many different concepts (many inspired by cognitive neuroscience). Behaviours are influenced and controlled by many mechanisms and so many methods are available in reinforcement learning. This section will provide some tips on exploring the many domains in RL, e.g. Exploration, Memory, Model-based. It is important to note the difference between “RL” and “deep RL”, the former being about traditional RL while the latter about applying neural networks to RL.</p> <h3 id="1-reinforcement-learning-an-introduction-by-richard-s-sutton-and-andrew-g-barto">1. <a href="http://incompleteideas.net/book/the-book.html">Reinforcement Learning: An Introduction</a> by Richard S. Sutton and Andrew G. Barto</h3> <p>Sutton and Barto is a good <strong>textbook</strong> on RL that is commonly coined the RL “bible” for its prominence. The textbook is a comprehensive resource and reference for RL, covering tabular methods such as multi-armed bandits, dynamic programming, Monte Carlo methods, TD-learning and bootstrapping, to approximate methods for prediction and control and policy gradients.</p> <h3 id="2-introduction-to-reinforcement-learning-by-david-silver-deepmind">2. <a href="https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver">Introduction to Reinforcement Learning</a> by David Silver, DeepMind</h3> <p>Intro to RL by David Silver, DeepMind is a good <strong>lecture</strong> series on reinforcement learning. It formulates RL with concepts about Markov decision processes, planning by dynamic programming, prediction vs control, value function approximation, among others. David Silver is a RL pioneer who co-authored <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a>, one of the first successful attempts to apply deep neural networks to reinforcement learning.</p> <h3 id="3-reinforcement-learning-specialization-on-coursera-by-university-of-alberta">3. <a href="https://www.coursera.org/specializations/reinforcement-learning">Reinforcement Learning Specialization on Coursera</a> by University of Alberta</h3> <p>The RL Specialization on Coursera is a good <strong>course</strong> on RL, introducing concepts such as temporal difference learning, Monte Carlo, Sarsa, Q-learning, policy gradients, the Dyna architecture, and more. The course is taught by Martha White and Adam White, who are students of Richard Sutton, the author of the Reinforcement Learning: An Introduction textbook (<a href="https://www.reddit.com/r/MachineLearning/comments/h940xb/what_is_the_best_way_to_learn_about_reinforcement/">who personally recommended the course</a>).</p> <h3 id="4-spinning-up-key-papers-in-deep-rl-by-openai">4. <a href="https://spinningup.openai.com/en/latest/spinningup/keypapers.html">Spinning Up: Key Papers in Deep RL</a> by OpenAI</h3> <p>Spinning Up by OpenAI is a resource that introduces deep RL concepts to people interested in picking up RL. Their Key Papers in Deep RL section provides a good starting point to the overview of RL topics, introducing milestone research papers in each topic. Examples: Model-Free RL, Exploration, Transfer and Multitask RL, Hierarchy, Memory, etc.</p> <h3 id="5-cs285-deep-reinforcement-learning-by-sergey-levine-uc-berkeley">5. <a href="http://rail.eecs.berkeley.edu/deeprlcourse/">CS285: Deep Reinforcement Learning</a> by Sergey Levine, UC Berkeley</h3> <p>CS285 is a good <strong>lecture</strong> series on deep reinforcement learning, with concepts covering policy gradients, actor-critics, model-based RL, exploration, offline RL, inverse RL and meta-learning in RL, among others. Offline RL is used to train an RL agent offline (i.e. collect a batch of interactions then training the agent on it). Inverse RL involves using interaction data from a human expert and is used to infer the RL policy underlying the human decision making process. Sergey Levine is a prominent RL researcher and his course provides many good insights into deep RL.</p> <h3 id="6-algorithms-for-decision-making-by-mykel-j-kochenderfer-tim-a-wheeler-and-kyle-h-wray">6. <a href="https://algorithmsbook.com">Algorithms for Decision Making</a> by Mykel J. Kochenderfer, Tim A. Wheeler and Kyle H. Wray</h3> <p>Algorithms for Decision Making is a good and updated (2022) <strong>textbook</strong> that serves as a modern perspective of the problems in the field of reinforcement learning. In particular, it delivers a detailed walkthrough of belief state planning and multi-agent systems compared to Sutton &amp; Barto, and contains beautiful illustrations for explanations, complete with code implementations in Julia.</p> <hr/> <h2 id="advanced-reinforcement-learning">Advanced Reinforcement Learning</h2> <p>Beyond the standard learning resources are the topics that are active and unsolved open research questions. At this point, the research community is your best friend. The following resources and advice are strategies I found useful in my own learning journey.</p> <h3 id="1-survey-research-papers">1. Survey Research Papers</h3> <p>While there are many research papers that are impossible to read them all, there are survey papers that provide an overview of the current research landscape and topics. These survey papers provide an outline of the field and importantly introduce terminology that you can use to further search for what you are interested in. For example, <a href="https://arxiv.org/pdf/cs/9605103.pdf">Reinforcement Learning: A Survey</a> (Kaelbling et al. 1996) surveys the field of RL from a computer science perspective. For a more updated survey on deep RL, refer to <a href="https://discovery.ucl.ac.uk/id/eprint/10083557/1/1708.05866v2.pdf">A Brief Survey of Deep Reinforcement Learning</a> (Arulkumaran et al. 2017). I am personally interested in model-based reinforcement learning, so the following surveys served me well: <a href="https://arxiv.org/abs/2006.16712">Model-based Reinforcement Learning: A Survey</a> (Moerland et al. 2020), <a href="https://arxiv.org/abs/2008.05598">Deep Model-Based Reinforcement Learning for High-Dimensional Problems, a Survey</a> (Plaat et al. 2020). If you are interested in other domains in RL such as exploration or memory, you can search for terms terms like “reinforcement learning exploration survey” or “reinforcement learning memory survey” on Google Search or <a href="https://scholar.google.com/">Google Scholar</a>.</p> <h3 id="2-research-blogs--articles">2. Research Blogs / Articles</h3> <p>Blog posts are my personal favourite for developing an initial understanding of the research landscape or particular research papers. One of the most popular blogs is <a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">Lilian Weng’s blog</a>, that covers many topics including RL, natural language processing, computer vision and representation learning. <a href="https://bair.berkeley.edu/blog/">Berkeley AI Research (BAIR)’s blog</a>, <a href="https://ai.googleblog.com/">Google AI Blog</a> and <a href="https://ai.facebook.com/blog/?page=1">Facebook AI Research (FAIR)’s blog</a> often blog about their RL papers. <a href="https://distill.pub">Distill</a> was a peer-reviewed journal (not a blog) that focuses on interactive articles about machine learning research topics, allowing for playable exploration on their articles.</p> <h3 id="3-youtube-channels">3. YouTube Channels</h3> <p>YouTube channels also provide good videos that explain papers or provide updates on the state of research. <a href="https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfew">Yannic Kilcher</a> is a well-known YouTuber for his videos that explain recent hotly discussed papers. <a href="https://www.youtube.com/channel/UCHB9VepY6kYvZjj0Bgxnpbw/videos">Henry AI Labs</a> provide weekly updates about AI in general. <a href="https://www.youtube.com/channel/UCMLtBahI5DMrt0NPvDSoIRQ">Machine Learning Street Talk</a> conducts interviews with researchers to discuss about their research.</p> <h3 id="4-online-communities">4. Online Communities</h3> <p>Various online communities are active for research discussions as well, such as communities on Twitter or Reddit. Online communities are important because they foster discussion, debate and ideation. Twitter (or “Academic Twitter”) is a highly active community because many researchers tweet about their research. A list of researchers to follow on Twitter: <a href="https://www.reddit.com/r/MachineLearning/comments/b6dwrd/d_best_current_twitter_accounts_to_follow_for/">here</a>, but do actively adjust your Twitter feed according to your research interests. <a href="https://www.reddit.com/r/MachineLearning/">r/MachineLearning</a> is an active community that posts interesting research papers or discussion topics about ML research in general with thought-provoking comments. Or you could start your own a reading group!</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>Reinforcement learning, as you now know, is such a vast and diverse field by nature that is still a highly active research area with many open problems. While RL is currently mainly used for ads, search and recommendation, RL holds great potential by unlocking automation of robotics, driverless vehicles and many other applications. With this article, I hope that it helps more people get into reinforcement learning and unlock the many things that can be done with RL. That said, again, this is an opinionated article based on my learning journey, and likely will change as I progress in my learning. If you have recommendations for resources or tips to include in this article, feel free to contact me at <a href="mailto:notesjet@gmail.com">notesjet@gmail.com</a>. Cheers!</p>]]></content><author><name></name></author><category term="research"/><summary type="html"><![CDATA[A curation of resources for reinforcement learning.]]></summary></entry><entry><title type="html">SwAV: Paper Summary</title><link href="/blog/2021/swav/" rel="alternate" type="text/html" title="SwAV: Paper Summary"/><published>2021-06-21T00:00:00+00:00</published><updated>2021-06-21T00:00:00+00:00</updated><id>/blog/2021/swav</id><content type="html" xml:base="/blog/2021/swav/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/swav/swav_gif.gif" data-zoomable=""/> </div> </div> <div class="caption"> Swapping Assignment between multiple Views of the same image (SwAV). </div> <h2 id="why-swav">Why SwAV?</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/swav/swav_archi.png" data-zoomable=""/> </div> </div> <div class="caption"> The SwAV architecture. </div> <p>SwAV (Swapping Assignments between multiple Views of the same image) is introduced in Unsupervised Learning of Visual Features by Contrasting Cluster Assignments.</p> <p>SwAV achieves 75.3% top-1 accuracy on ImageNet with ResNet-50 for linear models trained on frozen features, which is only 1.2% difference from the supervised method (76.5%), closing the gap between unsupervised and supervised learning representation of visual features.</p> <h2 id="about-swav">About SwAV</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/swav/swav_perf.png" data-zoomable=""/> </div> </div> <div class="caption"> Evaluation of SwAV on the ImageNet benchmark. </div> <p>SwAV is an unsupervised contrastive learning method that simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or “views”) of the same image, and uses a “swapped” prediction mechanism by predicting the code of a view from the representation of another view.</p> <p>SwAV can be trained with large and small batches and can scale to unlimited amounts of data. SwAV also does not require a large memory bank or a special momentum network, therefore is more memory efficient than previous contrastive methods.</p> <p>SwAV also introduces a new augmentation “multi-crop” that increases the number of views with no computational or memory overhead.</p> <h2 id="motive">Motive</h2> <ol> <li> <p>Recent contrastive learning methods for unsupervised visual representation learning uses:</p> <ul> <li> <p>A contrastive loss that compares pairs of images to push away semantically different images while pulling together semantically similar images</p> </li> <li> <p>Image augmentations that define invariances encoded in the features.</p> </li> </ul> </li> <li> <p>Instance discrimination (contrasting instances of image views) is not practical for all pairwise comparisons on a large dataset. Clustering-based methods thus approximate task by discriminating between clusters images with similar features instead of individual images.</p> </li> <li> <p>However, current clustering-based methods are computationally inefficient.</p> </li> <li> <p>Therefore, SwAV proposes a scalable, online clustering-based method, and a “swapped” prediction problem to learn visual features.</p> </li> </ol> <h2 id="swav-loss">SwAV Loss</h2> <p>SwAV can be interpreted as contrasting between multiple images views by comparing cluster assignments instead of their features. SwAV does this by computing a code from one view and predicting that code from the other view.</p> <ol> <li> <p>Given image \(x\), augment and obtain the 2 views’ image features \(z_t, z_s\).</p> </li> <li> <p>Match image features \(z_t, z_s\) to prototypes \(\{c_1,...,c_k\}\) to compute codes \(q_t,q_s\).</p> </li> <li> <p>Set up the “swapped” prediction problem with the loss function: \(L(z_t,z_s)= l(z_t,q_s) + l(z_s,q_t)\) where \(l(z,q) = \sum_k q_s^{(k)}log\,p_t^{(k)}\) measures fit between features \(z\) and code \(q\), where \(p_t^{(k)} = \frac{exp(\frac{1}{\tau}z_t^\intercal c_k)}{\sum_{k'}exp(\frac{1}{\tau}z_t^\intercal c_{k'})}\).</p> </li> <li> <p>Taking this loss over all images and pairs of data augmentations lead to: \(-\frac{1}{N}\sum_{n=1}^N \sum_{s,t\sim T} [\frac{1}{\tau}z_{nt}^\intercal Cq_{ns} + \frac{1}{\tau}z_{ns}^\intercal Cq_{nt} - log\sum_{k=1}^K exp(\frac{z_{nt}^\intercal}{\tau}) - log\sum_{k=1}^K exp(\frac{z_{ns}^\intercal}{\tau})]\)</p> </li> <li> <p>This loss is minimized with respect to prototypes \(C\) and parameters \(\theta\) of image encoder \(f_\theta\).</p> </li> </ol> <h2 id="swav-computing-codes-online">SwAV Computing Codes Online</h2> <p>SwAV clusters instances to the prototypes \(C\), and compute codes using prototypes \(C\) such that all instances are equally partitioned by the prototypes.</p> <ol> <li> <p>Given image features \(Z = [z_1,...,z_B]\), map them to prototypes \(C = [c_1,...,c_k]\), where the mapping (codes) is denoted by \(Q = [q_1,...,q_B]\).</p> </li> <li> <p>\(Q\) is optimized to maximized similarity between features and prototypes using \(max_{Q\in \mathcal{Q}} Tr(Q^\intercal C^\intercal Z) + \varepsilon H(Q)\) where \(H(Q) = -\sum_{i,j}Q_{ij}log\,Q_{ij}\) is the entropy function.</p> </li> <li> \[C^\intercal Z = \begin{bmatrix} c_1 \\ c_2 \end{bmatrix} \begin{bmatrix} z_1 &amp; z_2 &amp; z_3 \end{bmatrix} = \begin{bmatrix} c_1z_1 &amp; c_1z_2 &amp; c_1z_3 \\ c_2z_1 &amp; c_2z_2 &amp; c_2z_3 \end{bmatrix}\] </li> <li> \[Tr(\begin{bmatrix} q_{11} &amp; q_{21} \\ q_{12} &amp; q_{22} \\ q_{13} &amp; q_{23} \end{bmatrix}\begin{bmatrix} C^\intercal Z \end{bmatrix}) = q_{11}c_1z_1 + q_{21}c_2z_1 + q_{12}c_1z_2 + q_{22}c_2z_2 + q_{13}c_1z_3 + q_{23}c_2z_3\] </li> <li> <p>\(Q\) is a range of continuous values between 0 and 1. \(Q\) is 0 for the general case and close to 1 when a \(z\) representation is close to its prototype vector \(C\). This is because optimizing \(Q\) to maximise the trace \(Tr(Q^\intercal C^\intercal Z)\) results in the dot products where \(c\) and \(z\) are close together will take a bigger value. The values of \(q\) will try to be maximised.</p> </li> <li> <p>However, the maximization of values of \(q\) are regularized by \(H(Q) = -\sum_{i,j}Q_{ij}log\,Q_{ij}\). The closer \(q_{ij}\) is to 0, the bigger the \(log\,Q_{ij}\) value will be, and the maximum of \(Q_{ij}log\,Q_{ij}\) will be in the middle of 0 and 1. A higher entropy will give a more homogenous distribution of \(Q\).</p> </li> <li>\(Q\) is optimized using the Sinkhorn-Knopp algorithm.</li> </ol> <p>Reference: <a href="https://www.youtube.com/watch?v=M_DgS3XGeJc&amp;ab_channel=PyTorchLightning">SwAV Loss Deep Dive by Ananya Harsh Jha</a></p> <h2 id="sinkhorn-knopp-algorithm">Sinkhorn-Knopp Algorithm</h2> <p>The goal of optimal transport is to transform one probability distribution into another with minimal cost.</p> <ol> <li>For example, let us allocate desserts to people according to preferences while constraining portion sizes.</li> <li>Let \(r = (3,3,3,4,2,2,2,1)\), the portion size of dessert each person can eat (n-dimensional).</li> <li>Let \(c = (4,2,6,4,4)\), the amount of each dessert available (m-dimensional).</li> <li>Let \(U(r,c)=\{P\in \mathbb{R}_{&gt;0}^{n\times m} \vert P\textbf{1}_m=r, P^\intercal\textbf{1}_n=c \}\) be the set of positive \(n\times m\) matrices for which rows sum to \(r\) and columns sum to \(c\), which is the set of ways of allocating desserts to the people.</li> <li>Let \(M\) be the \((n\times m)\) cost (negative preference) matrix.</li> <li>The optimal transport problem is formally posed as \(d_M(r,c) = min_{P\in U(r,c)} \sum_{i,j} P_{ij}M_{ij}\) the optimal transport between \(r\) and \(c\).</li> <li>The Sinkhorn distance is \(d_M(r,c) = min_{P\in U(r,c)} \sum_{i,j} P_{ij}M_{ij}-\frac{1}{\lambda}h(P)\) where \(h(P)=-\sum_{i,j}P_{ij}\) is the information entropy of \(P\) that acts as regularization.</li> <li>The Sinkhorn-Knopp algorithm is an efficient method to obtain the optimal distribution matrix \(P_\lambda^*\) and the associated \(d_M^\lambda(r,c)\) based on the fact that elements of the optimal matrix are of the form \((P_\lambda^*)_{ij} = \alpha_i\beta_je^{-\lambda M_{ij}}\) with \(\alpha_1,...,\alpha_n\) and \(\beta_1,...,\beta_n\) are constants to ensure rows and columns sum to \(r\) and \(c\) respectively.</li> <li>The Sinkhorn-Knopp algorithm is basically <ol> <li>Initialise \(P_\lambda = e^{-\lambda M}\).</li> <li>Repeat 3-4 until convergence:</li> <li>Scale the rows such that row sums match \(r\).</li> <li>Scale the columns such that column sums match \(c\).</li> </ol> </li> </ol> <p>Reference: <a href="https://michielstock.github.io/posts/2017/2017-11-5-OptimalTransport/">Notes on Optimal Transport by Michiel Stock</a></p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># amount each of the 7 people can eat
</span><span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>  <span class="c1"># portions of the 5 desserts available
</span><span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>  <span class="c1"># (n x m) preferences matrix
</span><span class="n">M</span> <span class="o">=</span> <span class="o">-</span><span class="n">M</span>  <span class="c1"># converting preferences to cost</span></code></pre></figure> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="sh">"""</span><span class="s">
Sinkhorn-Knopp Algorithm
</span><span class="sh">"""</span>
<span class="k">def</span> <span class="nf">sinkhorn_knopp</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">lam</span><span class="p">):</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">M</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span> <span class="n">lam</span> <span class="o">*</span> <span class="n">M</span><span class="p">)</span>
    <span class="n">P</span> <span class="o">/=</span> <span class="n">P</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

    <span class="k">while</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">u</span> <span class="o">-</span> <span class="n">P</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span> <span class="o">&gt;</span> <span class="mf">1e-8</span><span class="p">:</span>  
        <span class="n">u</span> <span class="o">=</span> <span class="n">P</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">P</span> <span class="o">*=</span> <span class="p">(</span><span class="n">r</span> <span class="o">/</span> <span class="n">u</span><span class="p">).</span><span class="nf">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">P</span> <span class="o">*=</span> <span class="p">(</span><span class="n">c</span> <span class="o">/</span> <span class="n">P</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)).</span><span class="nf">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">P</span></code></pre></figure> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">P</span> <span class="o">=</span> <span class="nf">sinkhorn_knopp</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">lam</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">P</span><span class="p">).</span><span class="n">plot</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="n">stacked</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span></code></pre></figure> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/swav/swav_sinkhorn.png" data-zoomable=""/> </div> </div> <h2 id="why-swav-works">Why SwAV Works</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/swav/swav_deepcluster.png" data-zoomable=""/> </div> </div> <div class="caption"> SwAV compared to clustering-based approaches, e.g. DeepCluster-v2. </div> <ol> <li> <p>SwAV authors re-implemented and improved previous clustering-based models to compare with SwAV.</p> </li> <li> <p>DeepCluster-v2 obtains 75.2% top-1 accuracy on ImageNet versus 75.3% for SwAV.</p> </li> <li> <p>However, DeepCluster-v2 is not online, making it impractical for extremely large datasets, e.g. billion scale trainings which sometimes use only a single training epoch.</p> </li> <li> <p>As seen, SwAV can work online and therefore can scale better to unlimited amounts of data.</p> </li> </ol>]]></content><author><name></name></author><category term="research"/><summary type="html"><![CDATA[A paper summary of Unsupervised Learning of Visual Features by Contrasting Cluster Assignments by Mathilde et. al 2020.]]></summary></entry><entry><title type="html">Cross Entropy Method for Planning in Reinforcement Learning</title><link href="/blog/2021/cem/" rel="alternate" type="text/html" title="Cross Entropy Method for Planning in Reinforcement Learning"/><published>2021-05-31T00:00:00+00:00</published><updated>2021-05-31T00:00:00+00:00</updated><id>/blog/2021/cem</id><content type="html" xml:base="/blog/2021/cem/"><![CDATA[<h2 id="cross-entropy-method">Cross Entropy Method</h2> <p>The Cross Entropy Method (CEM) is a gradient-free method of optimization commonly used for planning in model-based reinforcement learning.</p> <h3 id="cem-algorithm">CEM Algorithm</h3> <ol> <li>Create a Gaussian distribution $N(\mu,\sigma)$ that describes the weights $\theta$ of the neural network.</li> <li>Sample $N$ batch samples of $\theta$ from the Gaussian.</li> <li>Evaluate all $N$ samples of $\theta$ using the value function, e.g. running trials.</li> <li>Select the top % of the samples of $\theta$ and compute the new $\mu$ and $\sigma$ to parameterise the new Gaussian distribution.</li> <li>Repeat steps 1-4 until convergence.</li> </ol> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">tensorflow_probability</span> <span class="k">as</span> <span class="n">tfp</span>
<span class="n">tfd</span> <span class="o">=</span> <span class="n">tfp</span><span class="p">.</span><span class="n">distributions</span>
<span class="kn">import</span> <span class="n">gym</span></code></pre></figure> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># RL Gym
</span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">CartPole-v0</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Initialisation
</span><span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># number of candidate policies
</span><span class="n">top_k</span> <span class="o">=</span> <span class="mf">0.40</span>  <span class="c1"># top % selected for next iteration
</span><span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># shape = (n_parameters, n_actions)
</span><span class="n">stddev</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># shape = (n_parameters, n_actions)</span></code></pre></figure> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">get_batch_weights</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">stddev</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">mvn</span> <span class="o">=</span> <span class="n">tfd</span><span class="p">.</span><span class="nc">MultivariateNormalDiag</span><span class="p">(</span>
        <span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span>
        <span class="n">scale_diag</span><span class="o">=</span><span class="n">stddev</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mvn</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">n</span><span class="p">).</span><span class="nf">numpy</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">policy</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">obs</span> <span class="o">@</span> <span class="n">weights</span><span class="p">[:</span><span class="mi">4</span><span class="p">,:]</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">run_trial</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">render</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="nf">policy</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
        <span class="n">obs</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">reward</span> <span class="o">+=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">render</span><span class="p">:</span>
            <span class="n">env</span><span class="p">.</span><span class="nf">render</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">reward</span>

<span class="k">def</span> <span class="nf">get_new_mean_stddev</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">batch_weights</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">rewards</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="nf">int</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">top_k</span><span class="p">)]</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">batch_weights</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">stddev</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">var</span><span class="p">(</span><span class="n">batch_weights</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">stddev</span></code></pre></figure> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">batch_weights</span> <span class="o">=</span> <span class="nf">get_batch_weights</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">stddev</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[</span><span class="nf">run_trial</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="k">for</span> <span class="n">weights</span> <span class="ow">in</span> <span class="n">batch_weights</span><span class="p">]</span>
    <span class="n">mean</span><span class="p">,</span> <span class="n">stddev</span> <span class="o">=</span> <span class="nf">get_new_mean_stddev</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">batch_weights</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span></code></pre></figure> <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">[</span>20.0, 10.0, 9.0, 16.0, 22.0, 10.0, 10.0, 10.0, 10.0, 9.0]
<span class="o">[</span>30.0, 56.0, 26.0, 125.0, 13.0, 9.0, 9.0, 114.0, 28.0, 8.0]
<span class="o">[</span>89.0, 111.0, 69.0, 9.0, 200.0, 69.0, 200.0, 105.0, 12.0, 31.0]
<span class="o">[</span>94.0, 128.0, 57.0, 30.0, 122.0, 107.0, 69.0, 37.0, 37.0, 141.0]
<span class="o">[</span>200.0, 200.0, 89.0, 200.0, 140.0, 91.0, 102.0, 149.0, 21.0, 81.0]
<span class="o">[</span>200.0, 154.0, 10.0, 112.0, 114.0, 187.0, 200.0, 200.0, 136.0, 149.0]
<span class="o">[</span>200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 149.0, 200.0, 200.0, 200.0]
<span class="o">[</span>200.0, 200.0, 134.0, 200.0, 200.0, 200.0, 180.0, 200.0, 200.0, 200.0]
<span class="o">[</span>200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]
<span class="o">[</span>200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 160.0, 131.0, 200.0]
<span class="o">[</span>200.0, 152.0, 163.0, 200.0, 153.0, 200.0, 200.0, 131.0, 200.0, 200.0]
<span class="o">[</span>200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]
<span class="o">[</span>200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]
<span class="o">[</span>200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]
<span class="o">[</span>200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]
<span class="o">[</span>200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]
<span class="o">[</span>200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]
<span class="o">[</span>200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]
<span class="o">[</span>200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]
<span class="o">[</span>200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]</code></pre></figure> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">mean</span><span class="p">,</span> <span class="n">stddev</span></code></pre></figure> <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">(</span>array<span class="o">([[</span><span class="nt">-0</span>.48842902, <span class="nt">-0</span>.20315496],
        <span class="o">[</span> 1.05925976,  1.55983425],
        <span class="o">[</span><span class="nt">-0</span>.83255259,  1.6572544 <span class="o">]</span>,
        <span class="o">[</span><span class="nt">-3</span>.46168438, <span class="nt">-0</span>.27580643],
        <span class="o">[</span> 0.16817479, <span class="nt">-0</span>.15037121]]<span class="o">)</span>,
 array<span class="o">([[</span>0.00026762, 0.00022525],
        <span class="o">[</span>0.00595117, 0.00055989],
        <span class="o">[</span>0.00042871, 0.09129609],
        <span class="o">[</span>0.00033094, 0.00030441],
        <span class="o">[</span>0.00055258, 0.00365766]]<span class="o">))</span></code></pre></figure> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">best_weights</span> <span class="o">=</span> <span class="nf">get_batch_weights</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">stddev</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span></code></pre></figure> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="nf">run_trial</span><span class="p">(</span><span class="n">best_weights</span><span class="p">,</span> <span class="n">render</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span></code></pre></figure> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">200.0</code></pre></figure>]]></content><author><name></name></author><category term="tutorial"/><category term="research"/><summary type="html"><![CDATA[A note on the Cross Entropy Method (CEM) commonly used for planning in reinforcement learning.]]></summary></entry><entry><title type="html">Why You Should Conduct a Technical Workshop</title><link href="/blog/2021/stats-soc-workshops/" rel="alternate" type="text/html" title="Why You Should Conduct a Technical Workshop"/><published>2021-03-19T00:00:00+00:00</published><updated>2021-03-19T00:00:00+00:00</updated><id>/blog/2021/stats-soc-workshops</id><content type="html" xml:base="/blog/2021/stats-soc-workshops/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/stats-soc-workshops/workshops.png" data-zoomable=""/> </div> </div> <div class="caption"> Workshops at NUS Statistics Society 2020/21 </div> <h2 id="what-is-a-technical-workshop">What is a technical workshop?</h2> <p>A technical workshop is a short lesson, like a lecture or tutorial, that usually emphasises hands-on practice into a technical tool or concept. In 2020 and 2021, <a href="https://sites.google.com/view/nusstatisticssociety/">NUS Statistics Society</a> conducted <a href="https://sites.google.com/view/nusstatisticssociety/workshops">8 technical workshops</a> in total for NUS students and the wider community in <a href="https://datascience.sg/">Data Science SG</a>. In this article, I share why I think you should experience conducting a technical workshop, based on my experience over 1 year directing the 9-member strong <a href="https://sites.google.com/view/nusstatisticssociety/about/about-workshops">Workshop Team</a> in the NUS Statistics Society 2020/21.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/stats-soc-workshops/pca-peng.jpg" data-zoomable=""/> </div> </div> <div class="caption"> All About PCA: Theory, Algorithm, Applications. Watch <a href="https://sites.google.com/view/nusstatisticssociety/workshops/all-about-pca-theory-algorithm-application">here</a>. </div> <h2 id="a-space-for-shared-curiosity">A space for shared curiosity</h2> <p>Workshops create an environment of shared curiosity and enthusiasm around a particular topic. If a topic piques your interest, you can safely ask questions to the presenters or fellow students. In Oct 2020, a workshop on <a href="https://sites.google.com/view/nusstatisticssociety/workshops/all-about-pca-theory-algorithm-application">Principal Component Analysis</a> (PCA), a machine learning algorithm, attracted 30 live students. Those who attended shared the doubts they had, and even raised questions that presenters haven’t thought of before. Other students joined in the discussion and came up with an answer, teaching something new to the presenter. The recursion of students asking questions to the teachers and teachers learning from students and back again reinforces a dynamic relationship that strengthens everyone’s learning in the topic. Creating a space where all learners can feel safe to learn new things is the main reason why I chose to direct Workshops at Statistics Society.</p> <h2 id="building-confidence-and-professionalism">Building confidence and professionalism</h2> <p>Workshops build a sense of confidence and professionalism that is robustly adaptable to unexpected situations. Each workshop member is responsible for the workshop that their group is conducting. In semester 1, everyone was assigned to workshops that were their “specialty”, namely the SQL (a database query language), applied ML (machine learning) and ML theory workshops. Given prior knowledge in what everyone is supposed to present, everyone can instead focus on the workshop conducting skills required to run a workshop effectively. Behind every workshop is a full rehearsal a few days before.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/stats-soc-workshops/sql-agatha.png" data-zoomable=""/> </div> </div> <div class="caption"> SQL for Data Scientists 101 &amp; 102. Watch <a href="https://sites.google.com/view/nusstatisticssociety/workshops/sql-for-data-scientists-101">here</a> and <a href="https://sites.google.com/view/nusstatisticssociety/workshops/sql-for-data-scientists-102">here</a>. </div> <h2 id="managing-the-unpredictability-of-live-events">Managing the unpredictability of live events</h2> <p>But inevitably, technical difficulties occur. During the <a href="https://sites.google.com/view/nusstatisticssociety/workshops/sql-for-data-scientists-101">SQL workshop</a> with a large crowd of 167 viewers, Agatha lost wifi connection, but Michael Yang stepped up to the occasion to take over the content his partner had prepared for the workshop (a full hour of content), given his prior knowledge in SQL and the familiarity having had a rehearsal with his partner. On the other hand, Agatha, despite having prepared hours of effort in the workshop, dealt with the inevitable with grace, by itself is an admirable feat. Technical issues are a nature of live and online events, and to overcome them is an ability that only those who have gone through them can acquire.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/stats-soc-workshops/facemask-georgie.png" data-zoomable=""/> </div> </div> <div class="caption"> Applied Machine Learning with Face Mask Detection. Watch <a href="https://sites.google.com/view/nusstatisticssociety/workshops/applied-machine-learning-for-face-mask-detection">here</a>. </div> <h2 id="a-sense-of-ownership">A sense of ownership</h2> <p>Workshops create a strong portfolio and ownership towards the conduct of the workshop and the end-product of the workshop. In the <a href="https://sites.google.com/view/nusstatisticssociety/workshops/applied-machine-learning-for-face-mask-detection">applied ML workshop</a> of 57 viewers, Georgie and Rama led in a hands-on demo to build an end-to-end face mask detection web app, including the initial machine learning training. Rama created a handy Jupyter notebook that compiled the training pipeline, from feature extraction, transfer learning, model training and evaluation, complete with elaborate documentation. Georgie constructed the backend to connect the face mask detector to a user-facing web app, with optimised latency to detect whether a person is wearing a mask in real time. The resources created by them are <a href="https://github.com/geoboom/facemask-workshop">freely available</a> and serves as a useful indicator to future employers that represent their capabilities.</p> <h2 id="on-leadership-as-a-necessity">On leadership as a necessity</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/stats-soc-workshops/leaders.jpg" data-zoomable=""/> </div> </div> <div class="caption"> A Leaders Outing to Lunch and Board Game Cafe. (FYI: Laws adhered) </div> <p>Leadership is one of the core necessities in any endeavour. Before directing workshops at NUS Statistics Society, I was a workshop member at <a href="https://dsc.comp.nus.edu.sg/">NUS Developer Student Club</a> in AY2019/2020, presenting <a href="https://www.youtube.com/playlist?list=PLiAp0_yuG0tY3bldy2K3L3s5XZmlmy8Gu">a series of 5 data science workshops</a>, including one at Google Developer Space.</p> <p>What made me make the switch to Statistics Society was the then and current president Joel Tan, who emphasised the flexibility of members to lead in their own style, anything from allowing us to decide the size of our team to the many new initiates we could try implementing over the year. This focus on member flexibility was shared across the society, and expressed in the many new initiatives we kick-started, such as <a href="https://www.instagram.com/nusstatssoc/">memes on Instagram</a>, <a href="https://sites.google.com/view/nusstatisticssociety/articles">publishing member-curated articles</a>, a workshop collaboration with the <a href="https://sites.google.com/view/nusstatisticssociety/workshops/effective-data-visualization">psychology department</a> and <a href="https://datascience.sg/">Data Science SG community</a>, badminton sessions, and a monthly internal paper sharing club within the society, among many others. With good leadership, a good experience in the CCA will naturally follow. If this interests you, we are <a href="https://nus.campuslabs.com/engage/submitter/form/start/473251">currently recruiting the next batch of leaders</a> in the society!</p> <h2 id="recruitment-as-a-commitment">Recruitment as a commitment</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/stats-soc-workshops/team-profile.png" data-zoomable=""/> </div> </div> <div class="caption"> The Workshops Team. </div> <p>During the recruitment for the workshop team, I faced a difficulty - 8 people made it through my interview (difficult as described by many), but most teams only had at most 5 members. Here is the dilemma: Having a large team can mean some members might not be meaningfully engaged if there wasn’t much work to do, while planning more work to do means that much more responsibility and organisation was required by me to commit to over the year. After much consideration, I decided against randomly selecting an arbitrarily smaller number of members, as it was unfair to those who did qualify but were not selected, accepted all 8 people, and readied to commit to more work. With a large 9-man strong team, the amount of work was great, but I believe the team felt was especially fulfilling as we conducted 8 impactful workshops with over 800 peak unique concurrent workshop participants, across the short span of a year!</p> <h2 id="experience-over-outcomes">Experience over outcomes</h2> <p>As a society, the shared value across the society emphasises on the experience of members. However, due to the COVID-19 pandemic, there were very few opportunities to meet up in person, much less for team bonding. In fact, all meetings in semester 1 were conducted online and all workshops across the year were conducted online.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/stats-soc-workshops/team-dinner.png" data-zoomable=""/> </div> </div> <div class="caption"> Workshops Team Christmas Pizza Dinner. </div> <p>As a commitment towards a good experience for members, we allocated a part of our societal budget to subsidise team lunches once a semester, which helped with bonding in the society. At the end of 2020, when COVID-19 restrictions were reduced, we went for our first in-person team dinner. Nonetheless, what I think makes a bonded team is not the benefits that an organisation provides, but the willingness of each member to commit to the team. The purpose of joining a student organisation has never been for monetary gain or reputational benefits (there really isn’t much). It is not to achieve a certain outcomes, e.g. “I want to execute X number of workshops this year”, but rather to subscribe to a fun and enjoyable experience for everyone. Despite many COVID-19 difficulties, we persisted and forged many strong friendships across the society.</p> <h2 id="tips-on-planning-a-workshop">Tips on planning a workshop</h2> <p>Having organised a year’s worth of workshops, here are some tips I’ve learnt about planning for a workshop and what I would change if I could go back in time.</p> <p>Firstly, and arguably most importantly, workshops should aim to be beginner-friendly and have as few pre-requisites as possible. Workshops are not meant to be a crash course – they are meant to be an introductory taste to pique the learners’ interest and curiosity to further explore the subject on their own. This is what I would improve on if I could turn back time. Some of our workshops were admittedly not beginner-friendly, leading some students to be lost midway in the workshop.</p> <p>Secondly, workshops should be timed near the start of the academic semester, between weeks 1 to 3, or a less ideal but still alright recess week. In all other weeks, students are focusing on assignment deadlines or exams, and will not turn up even if they do register for the event. An early workshop in the semester means the term break prior to the semester should be well-used for workshop preparation.</p> <p>Thirdly, publicise your workshop by showing how your target audience will benefit from it. For example, students should learn SQL because it is highly relevant in many jobs from software engineers to data scientists. Showing the relevance of the workshop content will help potential participants understand how joining the workshop will be a good use of their time.</p> <p>Fourthly, involve your participants with some form of hands-on activity. Spend some buffer time to let participants play around with an API or attempt to solve problems. In our SQL workshop, since learning SQL requires practice, we introduced sample SQL questions and allocated time for students to try their hand at them, and the feedback showed that the hands-on practice benefitted the students.</p> <p>Lastly, try to keep the duration of the workshop to at most 1.5 hours. Most of the learners, even the presenter, will begin to tire and feel that the workshop is dragging on by the 1.5-hour mark.</p> <p>If this has piqued your interest, we are also recruiting members for the next academic year very soon!</p> <h2 id="concluding">Concluding</h2> <p>All in all, conducting a technical workshop is a great means of professional development in one’s career, but the biggest takeaway is definitely the friends you meet and the memories forged. The past year has been extremely eventful and filled with challenging decisions, and I will continue to remember them for a long time.</p> <p>To connect with us, find us on <a href="http://sites.google.com/view/nusstatisticssociety">our website</a>, <a href="http://tinyurl.com/StatsTele">Telegram channel, <a href="http://tinyurl.com/StatsTele">Instagram</a>, <a href="http://linkedin.com/company/nusstatssoc/">LinkedIn</a> or our <a href="mailto:statistics.society@u.nus.edu">society email</a>.</a></p> <hr/> <h3 id="about-the-author">About the Author</h3> <p>Jet New is the Director of Workshops at NUS Statistics Society AY2020/21. He studies computer science + USP and researches on causal reinforcement learning.</p> <h3 id="workshop-statistics">Workshop Statistics</h3> <table style="width:100%"> <tr> <th>Workshop</th> <th>Registrations</th> <th>Peak Concurrent Viewers</th> <th>Feedback (Submissions)</th> </tr> <tr> <td>Machine Learning from Scratch</td> <td>NA</td> <td>60</td> <td>-</td> </tr> <tr> <td>SQL for Data Scientists 101</td> <td>470</td> <td>167</td> <td>4.93 (15)</td> </tr> <tr> <td>SQL for Data Scientists 102</td> <td>470</td> <td>51</td> <td>4.90 (10)</td> </tr> <tr> <td>Applied Machine Learning for Face Mask Detection</td> <td>183</td> <td>57</td> <td>4.50 (10)</td> </tr> <tr> <td>All About PCA: Theory, Algorithm, Applications</td> <td>72</td> <td>30</td> <td>4.71 (7)</td> </tr> <tr> <td>Data Science Competition: Applied Computer Vision</td> <td>NA</td> <td>415</td> <td>4.31 (13)</td> </tr> <tr> <td>Effective Data Storytelling</td> <td>88</td> <td>41</td> <td>4.45 (11)</td> </tr> <tr> <td>Using Tidyverse in R for Data Analysis and Visualisation</td> <td>83</td> <td>27</td> <td>4.33 (3)</td> </tr> </table>]]></content><author><name></name></author><category term="life"/><summary type="html"><![CDATA[Reflections on leading the Workshops Team at NUS Statistics Society in AY2020/21.]]></summary></entry><entry><title type="html">Evolutionary Computation: Genetic Algorithm, Neuroevolution and Novelty Search</title><link href="/blog/2020/evo-comp/" rel="alternate" type="text/html" title="Evolutionary Computation: Genetic Algorithm, Neuroevolution and Novelty Search"/><published>2020-11-28T00:00:00+00:00</published><updated>2020-11-28T00:00:00+00:00</updated><id>/blog/2020/evo-comp</id><content type="html" xml:base="/blog/2020/evo-comp/"><![CDATA[<h2 id="genetic-algorithm">Genetic Algorithm</h2> <p>The genetic algorithm is a nature-inspired algorithm based on natural selection, that the fittest individuals of a population are selected to reproduce the next generation.</p> <p>The genetic algorithm consists of 5 processes:</p> <ol> <li>Initial population</li> <li>Fitness function</li> <li>Selection</li> <li>Crossing-over</li> <li>Mutation</li> </ol> <p>Terminology:</p> <ul> <li>Population refers to the set of individuals (solution).</li> <li>Individual is defined by its chromosome (set of parameters/variables).</li> <li>Fitness function refers to the performance measure of an individual.</li> <li>Selection refers to the selection of the fittest.</li> <li>Crossing-over refers to a swapping of segments of 2 parents’ genes, producing a child individual with a new gene combination.</li> <li>Mutation is a random perturbation of genes based on a probability.</li> </ul> <h3 id="optimization-problem-linear-regression">Optimization Problem: Linear Regression</h3> <p>Evolutionary algorithms can serve as “black box” optimisation algorithms without needing to solving the objective function analytically. To illustrate that evolutionary algorithms can optimise, the simple linear regression problem is used. Define a linear function:</p> \[y = mx + c + \epsilon\] <p>to be modelled by a linear regression model, where $m=1$, $c=0$, $\epsilon\sim N(0,1)$ represents gradient, y-intercept and Gaussian noise respectively.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># for reproducibility
</span></code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Linear function with Gaussian noise</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/evo-comp/linear_target.png" data-zoomable=""/> </div> </div> <div class="caption"> Linear target function. </div> <h3 id="process-1-generate-the-initial-population-of-individuals">Process 1: Generate the initial population of individuals.</h3> <p>Each individual (solution/model) is defined by a set of parameters. Hyperparameters to be specified, which are variables that are not updated at every iteration of optimisation, are the population size (number of individuals in the population at any point in time) and the number of parameters that defines an individual. The initial population’s parameters can be zero-initialised or random-initialised. For your interest, there also exists many other initialisation methods to be used depending on context, such as the He initialisation and Xavier initialisation. The set of parameters that defines each individual is biologically analogous to the individual’s genome (or gene or chromosome, depending on the computational process).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">population_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">num_parameters</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># initial_population = np.zeros(shape=(population_size, num_parameters))  # zero initialisation
</span><span class="n">initial_population</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">population_size</span><span class="p">,</span> <span class="n">num_parameters</span><span class="p">))</span>  <span class="c1"># random normal initialisation
</span><span class="n">initial_population</span>


<span class="nf">array</span><span class="p">([[</span> <span class="mf">1.8831507</span> <span class="p">,</span> <span class="o">-</span><span class="mf">1.34775906</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.270485</span>  <span class="p">,</span>  <span class="mf">0.96939671</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.17312341</span><span class="p">,</span>  <span class="mf">1.94362119</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.41361898</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.74745481</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">1.92294203</span><span class="p">,</span>  <span class="mf">1.48051479</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">1.86755896</span><span class="p">,</span>  <span class="mf">0.90604466</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.86122569</span><span class="p">,</span>  <span class="mf">1.91006495</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.26800337</span><span class="p">,</span>  <span class="mf">0.8024564</span> <span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.94725197</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.15501009</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.61407937</span><span class="p">,</span>  <span class="mf">0.92220667</span><span class="p">]])</span>
</code></pre></div></div> <h3 id="process-2-compute-the-fitness-of-all-individuals">Process 2: Compute the fitness of all individuals.</h3> <p>Another 2 hyperparameters are in the form of functions - the solution and the fitness function. The solution is a model that uses the individual’s parameters to compute the output $y$ given input $X$. For simplicity, we use the polynomial regression model (with 2 parameters, it is a simple linear regression model):</p> \[f(X) = \theta_1 X + \theta_2\] <p>where $\theta_1$ and $\theta_2$ should converge to $m$ and $c$ respectively eventually. The fitness function measures the performance of an individual solution. The evolutionary analogy of the fitness function of an organism would be, for example, its survivability and/or reproductive success. Because we want to model the linear function with Gaussian noise dataset, the negative mean squared error (MSE) is used as the fitness function to determine how well the solution models the dataset:</p> \[MSE = \frac{1}{n} \sum_{i=1}^n (y_{i} - f(X_i))^2\] <p>Because the fitness function is to be maximised, MSE is negated to reflect a higher value of MSE as more desirable.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">solution</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>  <span class="c1"># Polynomial regression model
</span>  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">([</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">params</span><span class="p">))],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">fitness_function</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>  <span class="c1"># Mean squared error
</span>  <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="nf">abs</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="nf">solution</span><span class="p">(</span><span class="n">params</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_data</span><span class="p">():</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_individual</span><span class="p">(</span><span class="n">individual</span><span class="p">):</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="nf">solution</span><span class="p">(</span><span class="n">individual</span><span class="p">),</span> <span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="sh">'</span><span class="s">grey</span><span class="sh">'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_population</span><span class="p">(</span><span class="n">population</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">individual</span> <span class="ow">in</span> <span class="n">population</span><span class="p">:</span>
    <span class="nf">plot_individual</span><span class="p">(</span><span class="n">individual</span><span class="p">)</span>

<span class="n">individual</span> <span class="o">=</span> <span class="n">initial_population</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">fitness_score</span> <span class="o">=</span> <span class="nf">fitness_function</span><span class="p">(</span><span class="n">individual</span><span class="p">)</span>

<span class="nf">plot_data</span><span class="p">()</span>
<span class="nf">plot_individual</span><span class="p">(</span><span class="n">individual</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="nf">plot_data</span><span class="p">()</span>
<span class="nf">plot_population</span><span class="p">(</span><span class="n">initial_population</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/evo-comp/individual_solution.png" data-zoomable=""/> </div> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/evo-comp/initial_population.png" data-zoomable=""/> </div> </div> <div class="caption"> Individual candidate solution (left) and initial population (right). </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_fitness</span><span class="p">(</span><span class="n">population</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="nf">fitness_function</span><span class="p">(</span><span class="n">individual</span><span class="p">)</span> <span class="k">for</span> <span class="n">individual</span> <span class="ow">in</span> <span class="n">population</span><span class="p">])</span>

<span class="n">fitness_scores</span> <span class="o">=</span> <span class="nf">compute_fitness</span><span class="p">(</span><span class="n">initial_population</span><span class="p">)</span>
<span class="n">fitness_scores</span>


<span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mf">145.00615079</span><span class="p">,</span>   <span class="o">-</span><span class="mf">3.20852429</span><span class="p">,</span>  <span class="o">-</span><span class="mf">21.20939814</span><span class="p">,</span> <span class="o">-</span><span class="mf">110.93013108</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">21.41800978</span><span class="p">,</span>   <span class="o">-</span><span class="mf">2.83355152</span><span class="p">,</span>  <span class="o">-</span><span class="mf">21.68891802</span><span class="p">,</span>   <span class="o">-</span><span class="mf">2.97834017</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">35.66225857</span><span class="p">,</span>   <span class="o">-</span><span class="mf">1.05527391</span><span class="p">])</span>
</code></pre></div></div> <h3 id="process-3-select-the-fittest-individuals">Process 3: Select the fittest individuals.</h3> <p>Like natural selection, select the top $k$ percentage of individuals with the highest fitness scores, where $k$ is a hyperparameter, to form the parent subpopulation that will reproduce to form the next generation of the population later.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_fittest</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">fitness_scores</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">population</span><span class="p">[</span><span class="n">fitness_scores</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(),</span> <span class="p">:]</span>

<span class="k">def</span> <span class="nf">select_fittest</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">fitness_scores</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">population</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">fitness_scores</span><span class="p">)[</span><span class="o">-</span><span class="nf">int</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">population</span><span class="p">)</span> <span class="o">*</span> <span class="n">k</span><span class="p">):],</span> <span class="p">:]</span>

<span class="n">parent_subpopulation</span> <span class="o">=</span> <span class="nf">select_fittest</span><span class="p">(</span><span class="n">initial_population</span><span class="p">,</span> <span class="n">fitness_scores</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">parent_subpopulation</span><span class="p">,</span> <span class="nf">compute_fitness</span><span class="p">(</span><span class="n">parent_subpopulation</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(array([[1.86755896, 0.90604466],
        [0.61407937, 0.92220667]]), array([-2.83355152, -1.05527391]))
</code></pre></div></div> <h3 id="process-4-perform-crossing-over-between-parents-to-produce-children">Process 4: Perform crossing-over between parents to produce children.</h3> <p>Crossing-over is a biological process that exchanges genetic material to result in new combinations of genetic material. For the benefit of non-biology students, much detail has been abstracted out and interested readers can refer to chromosomal crossovers. In the genetic algorithm, crossing-over is performed during reproduction by swapping a segment of parameters of one parent with another parent. For example, take 2 parents defined by 4 parameters:</p> \[P1 = [A1, A2, A3, A4], P2 = [B1, B2, B3, B4]\] <p>A crossing-over at the index 3 will result in a child:</p> \[C = [A1, A2, B3, B4]\] <p>There exists other methods of genetic exchange to introduce variance in the population gene pool, such as swapping elements instead of segments.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">perform_crossingover</span><span class="p">(</span><span class="n">subpopulation</span><span class="p">):</span>
  <span class="n">children</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">population_size</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">subpopulation</span><span class="p">)):</span>
    <span class="n">parents</span> <span class="o">=</span> <span class="n">subpopulation</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">subpopulation</span><span class="p">),</span> <span class="mi">2</span><span class="p">)]</span>
    <span class="n">gene_indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">num_parameters</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">gene_indices</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">gene_indices</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">):]</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># segment swap
</span>    <span class="n">child</span> <span class="o">=</span> <span class="n">parents</span><span class="p">[</span><span class="n">gene_indices</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">num_parameters</span><span class="p">)]</span>
    <span class="n">children</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">subpopulation</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">children</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">next_population</span> <span class="o">=</span> <span class="nf">perform_crossingover</span><span class="p">(</span><span class="n">parent_subpopulation</span><span class="p">)</span>
<span class="n">next_population</span><span class="p">,</span> <span class="nf">compute_fitness</span><span class="p">(</span><span class="n">next_population</span><span class="p">)</span>


<span class="p">(</span><span class="nf">array</span><span class="p">([[</span><span class="mf">1.86755896</span><span class="p">,</span> <span class="mf">0.90604466</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.61407937</span><span class="p">,</span> <span class="mf">0.92220667</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.61407937</span><span class="p">,</span> <span class="mf">0.92220667</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.86755896</span><span class="p">,</span> <span class="mf">0.90604466</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.61407937</span><span class="p">,</span> <span class="mf">0.92220667</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.61407937</span><span class="p">,</span> <span class="mf">0.92220667</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.86755896</span><span class="p">,</span> <span class="mf">0.92220667</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.61407937</span><span class="p">,</span> <span class="mf">0.92220667</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.86755896</span><span class="p">,</span> <span class="mf">0.92220667</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.61407937</span><span class="p">,</span> <span class="mf">0.92220667</span><span class="p">]]),</span>
  <span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mf">2.83355152</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.05527391</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.05527391</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.83355152</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.05527391</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">1.05527391</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.04089715</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.05527391</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.04089715</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.05527391</span><span class="p">]))</span>
</code></pre></div></div> <h3 id="process-5-perform-mutation-on-the-population">Process 5: Perform mutation on the population.</h3> <p>A mutation is defined as a change in the DNA sequence. While the exact differences between DNA, gene and chromosome in the genetic algorithm are not maintained, inspiration is drawn from mutation in biology that usually worsens fitness but can occasionally improve fitness of the individual. To perform mutation on the population parameters, add Gaussian noise $\epsilon\sim N(0, \sigma)$ to the individuals’ parameters, where $\sigma$ is the standard deviation hyperparameter.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">perform_mutation</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">population</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">population</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Gaussian noise
</span>
<span class="n">mutated_population</span> <span class="o">=</span> <span class="nf">perform_mutation</span><span class="p">(</span><span class="n">next_population</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">mutated_population</span><span class="p">,</span> <span class="nf">compute_fitness</span><span class="p">(</span><span class="n">mutated_population</span><span class="p">)</span>


<span class="p">(</span><span class="nf">array</span><span class="p">([[</span><span class="mf">1.86732417</span><span class="p">,</span> <span class="mf">0.90693888</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.6081361</span> <span class="p">,</span> <span class="mf">0.9211497</span> <span class="p">],</span>
        <span class="p">[</span><span class="mf">0.62150733</span><span class="p">,</span> <span class="mf">0.91465686</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.86617613</span><span class="p">,</span> <span class="mf">0.88479546</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.60377918</span><span class="p">,</span> <span class="mf">0.91703215</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.61266137</span><span class="p">,</span> <span class="mf">0.90944187</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.88237409</span><span class="p">,</span> <span class="mf">0.93879944</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.61408077</span><span class="p">,</span> <span class="mf">0.93726506</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.86860417</span><span class="p">,</span> <span class="mf">0.91984624</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.62227372</span><span class="p">,</span> <span class="mf">0.92623524</span><span class="p">]]),</span>
  <span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mf">2.84393595</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0525611</span> <span class="p">,</span> <span class="o">-</span><span class="mf">1.05282308</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.58416917</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.04907953</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">1.04977737</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.3166936</span> <span class="p">,</span> <span class="o">-</span><span class="mf">1.07545786</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.01246558</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0622913</span> <span class="p">]))</span>
</code></pre></div></div> <h3 id="the-genetic-algorithm-all-5-processes-together">The Genetic Algorithm: All 5 Processes Together</h3> <p>By combining the 5 processes together, we construct the genetic algorithm and run it to find a solution that models the linear function well.</p> <p>Genetic Algorithm:</p> <ol> <li>Generate the initial population of individuals.</li> <li>Repeat until convergence: <ol> <li>Compute fitness of the population.</li> <li>Select the fittest individuals (parent subpopulation).</li> <li>Perform crossing-over between parents to produce children.</li> <li>Perform mutation on the population.</li> </ol> </li> <li>Select the fittest individual of the population as the solution.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define hyperparameters of the genetic algorithm.
</span><span class="n">population_size</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">num_parameters</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">num_generations</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">top_k</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">mutation_sigma</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="c1"># Process 1: Generate the initial population of individuals.
</span><span class="n">population</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">population_size</span><span class="p">,</span> <span class="n">num_parameters</span><span class="p">))</span>

<span class="c1"># Misc: Experimental tracking
</span><span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">solutions</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Iterate the process over multiple generations of populations.
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_generations</span><span class="p">):</span>

  <span class="c1"># Process 2: Compute the fitness of all individuals.
</span>  <span class="n">fitness_scores</span> <span class="o">=</span> <span class="nf">compute_fitness</span><span class="p">(</span><span class="n">population</span><span class="p">)</span>

  <span class="c1"># Process 3: Select the fittest individuals.
</span>  <span class="n">fittest_subpopulation</span> <span class="o">=</span> <span class="nf">select_fittest</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">fitness_scores</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">top_k</span><span class="p">)</span>
  
  <span class="c1"># Misc: Experimental tracking
</span>  <span class="n">fittest</span> <span class="o">=</span> <span class="nf">get_fittest</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">fitness_scores</span><span class="p">)</span>
  <span class="n">solutions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">solution</span><span class="p">(</span><span class="n">fittest</span><span class="p">))</span>
  <span class="n">scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">fitness_function</span><span class="p">(</span><span class="n">fittest</span><span class="p">))</span>

  <span class="c1"># Process 4: Perform crossing-over between parents to produce children.
</span>  <span class="n">children</span> <span class="o">=</span> <span class="nf">perform_crossingover</span><span class="p">(</span><span class="n">fittest_subpopulation</span><span class="p">)</span>

  <span class="c1"># Process 5: Perform mutation on the population.
</span>  <span class="n">population</span> <span class="o">=</span> <span class="nf">perform_mutation</span><span class="p">(</span><span class="n">children</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">mutation_sigma</span><span class="p">)</span>


<span class="c1"># Misc: Experimental tracking
</span><span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">num_generations</span><span class="p">),</span> <span class="n">scores</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/evo-comp/ga_training_plot.png" data-zoomable=""/> </div> </div> <div class="caption"> Score over generations. </div> <h3 id="experiment-result">Experiment Result</h3> <p>The fittest individual in the final population is a reasonably well-fit linear regression model. The rest of the population have a lower fitness score but are quite well-fit as well.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fitness_score</span> <span class="o">=</span> <span class="nf">fitness_function</span><span class="p">(</span><span class="n">fittest</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="nf">solution</span><span class="p">(</span><span class="n">fittest</span><span class="p">)</span>

<span class="nf">plot_data</span><span class="p">()</span>
<span class="nf">plot_individual</span><span class="p">(</span><span class="n">fittest</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="nf">plot_data</span><span class="p">()</span>
<span class="nf">plot_population</span><span class="p">(</span><span class="n">fittest_subpopulation</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/evo-comp/individual_trained.png" data-zoomable=""/> </div> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/evo-comp/population_trained.png" data-zoomable=""/> </div> </div> <div class="caption"> Individual fitted solution (left) and fitted population (right). </div> <p>By visualising the fittest model at each generation (iteration) of the genetic algorithm, notice that virtually instantly, the linear regression model fits to the dataset. In fact, linear regression is too simple a problem to realise the effectiveness of the genetic algorithm. Nonetheless, the reason for using linear regression is to bring focus to the genetic algorithm without the overhead of needing to understand the model. For a more complex application of the genetic algorithm using neural networks, refer to the second section Neuroevolution. For an evolutionary strategy based on novelty applied on reinforcement learning, refer to the third section.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">capture</span>
<span class="kn">from</span> <span class="n">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">()</span>

<span class="nf">plot_data</span><span class="p">()</span>
<span class="n">ga_line</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">([],</span> <span class="p">[])[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="nf">max</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="nf">max</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">animate</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
  <span class="n">ga_line</span><span class="p">.</span><span class="nf">set_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">solutions</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
  <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Gen </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">ga_line</span><span class="p">,</span> <span class="n">ax</span>

<span class="n">ani</span> <span class="o">=</span> <span class="nc">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">animate</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_generations</span><span class="p">),</span> <span class="n">interval</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="sh">'</span><span class="s">/images/genetic-algorithm/genetic_algorithm.gif</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/evo-comp/genetic_algorithm.gif" data-zoomable=""/> </div> </div> <div class="caption"> Genetic algorithm on linear regression over generations. </div> <hr/> <h2 id="neuroevolution">Neuroevolution</h2> <p>Neuroevolution is a method of applying evolutionary algorithms to optimise neural networks instead of using backpropagation. Neuroevolution therefore is a non-gradient (or derivation-free) optimisation, which can speed up training as backward passes are not computed. The neural network optimised by neuroevolution can be adapted in terms of parameters, hyperparameters or network architecture. Prominent examples of neuroevolution are NeuroEvolution of Augmenting Topologies (NEAT) and Covariance-Matrix Adaptation Evolution Strategy (CMA-ES). The evolutionary algorithm employed in this notebook is the vanilla genetic algorithm without crossing-over, applying only mutation over neural network parameters (weights).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="n">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
</code></pre></div></div> <h3 id="the-neural-network-model-neuro-evolution">The Neural Network Model (“Neuro”-evolution)</h3> <p>The neural network, or a multi-layer perceptron, is a universal function approximator. The neural network in PyTorch with 2 hidden layers and non-linear activation functions hyperbolic tangent (tanh) and sigmoid is defined.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Sigmoid</span><span class="p">()</span>
<span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">tanh1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sigmoid</span><span class="p">()</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">tanh1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="n">net</span> <span class="o">=</span> <span class="nc">Net</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <h3 id="the-mutation-function-neuro-evolution">The Mutation Function (Neuro-“evolution”)</h3> <p>As with the genetic algorithm, neuroevolution can be implemented by adding an additive Gaussian noise $\epsilon\sim N(0,\sigma)$ to all neural network weights to introduce variance in the “gene pool” of the population.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torch.nn.utils</span> <span class="kn">import</span> <span class="n">parameters_to_vector</span><span class="p">,</span> <span class="n">vector_to_parameters</span>

<span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="n">net</span><span class="p">):</span>
  <span class="k">return</span> <span class="nf">parameters_to_vector</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>

<span class="k">def</span> <span class="nf">mutate_params</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="n">mutated_params</span> <span class="o">=</span> <span class="nf">get_params</span><span class="p">(</span><span class="n">net</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nf">get_params</span><span class="p">(</span><span class="n">net</span><span class="p">).</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nf">vector_to_parameters</span><span class="p">(</span><span class="n">mutated_params</span><span class="p">,</span> <span class="n">net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Before mutation:</span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="nf">get_params</span><span class="p">(</span><span class="n">net</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">mutate_params</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">After mutation:</span><span class="se">\n</span><span class="s"> </span><span class="si">{</span><span class="nf">get_params</span><span class="p">(</span><span class="n">net</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="n">Before</span> <span class="n">mutation</span><span class="p">:</span>
  <span class="nf">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5949</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1591</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6217</span><span class="p">,</span>  <span class="mf">0.0710</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6687</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3964</span><span class="p">,</span>  <span class="mf">0.3319</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2988</span><span class="p">,</span>
          <span class="mf">0.6695</span><span class="p">,</span>  <span class="mf">0.4645</span><span class="p">,</span>  <span class="mf">0.2398</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2250</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5464</span><span class="p">,</span>  <span class="mf">0.2512</span><span class="p">,</span>  <span class="mf">0.0582</span><span class="p">,</span>  <span class="mf">0.0818</span><span class="p">,</span>
          <span class="mf">0.1810</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5316</span><span class="p">,</span>  <span class="mf">0.3275</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1162</span><span class="p">,</span>  <span class="mf">0.2542</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6751</span><span class="p">,</span>  <span class="mf">0.4344</span><span class="p">,</span>  <span class="mf">0.1846</span><span class="p">,</span>
          <span class="mf">0.4996</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1422</span><span class="p">,</span>  <span class="mf">0.3201</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0814</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1195</span><span class="p">,</span>  <span class="mf">0.1880</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2272</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4236</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">0.0218</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6078</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0099</span><span class="p">,</span>  <span class="mf">0.1856</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4883</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2465</span><span class="p">,</span>  <span class="mf">0.0166</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1269</span><span class="p">,</span>
          <span class="mf">0.4119</span><span class="p">,</span>  <span class="mf">0.0229</span><span class="p">,</span>  <span class="mf">0.2381</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0007</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2959</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4865</span><span class="p">,</span>  <span class="mf">0.0240</span><span class="p">,</span>  <span class="mf">0.0228</span><span class="p">,</span>
          <span class="mf">0.2293</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0649</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1661</span><span class="p">,</span>  <span class="mf">0.0788</span><span class="p">,</span>  <span class="mf">0.2253</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1549</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2465</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0267</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">0.1861</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2189</span><span class="p">,</span>  <span class="mf">0.0964</span><span class="p">,</span>  <span class="mf">0.0684</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0555</span><span class="p">,</span>  <span class="mf">0.0063</span><span class="p">,</span>  <span class="mf">0.1374</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1588</span><span class="p">,</span>
          <span class="mf">0.2334</span><span class="p">],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">CatBackward</span><span class="o">&gt;</span><span class="p">)</span>

<span class="n">After</span> <span class="n">mutation</span><span class="p">:</span>
  <span class="nf">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.6005</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2099</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5364</span><span class="p">,</span>  <span class="mf">0.1007</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5897</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5340</span><span class="p">,</span>  <span class="mf">0.3688</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3615</span><span class="p">,</span>
          <span class="mf">0.7335</span><span class="p">,</span>  <span class="mf">0.3900</span><span class="p">,</span>  <span class="mf">0.2519</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1144</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5839</span><span class="p">,</span>  <span class="mf">0.2251</span><span class="p">,</span>  <span class="mf">0.0043</span><span class="p">,</span>  <span class="mf">0.1630</span><span class="p">,</span>
          <span class="mf">0.1419</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6593</span><span class="p">,</span>  <span class="mf">0.3079</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1238</span><span class="p">,</span>  <span class="mf">0.3217</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7810</span><span class="p">,</span>  <span class="mf">0.4419</span><span class="p">,</span>  <span class="mf">0.3621</span><span class="p">,</span>
          <span class="mf">0.5246</span><span class="p">,</span>  <span class="mf">0.0064</span><span class="p">,</span>  <span class="mf">0.4284</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1177</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0700</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0537</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1281</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3613</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">0.0873</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6996</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1507</span><span class="p">,</span>  <span class="mf">0.1944</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6326</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1384</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0384</span><span class="p">,</span>  <span class="mf">0.0323</span><span class="p">,</span>
          <span class="mf">0.3344</span><span class="p">,</span>  <span class="mf">0.0667</span><span class="p">,</span>  <span class="mf">0.1177</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1347</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3413</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5302</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1326</span><span class="p">,</span>  <span class="mf">0.3330</span><span class="p">,</span>
          <span class="mf">0.2282</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1485</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1944</span><span class="p">,</span>  <span class="mf">0.2058</span><span class="p">,</span>  <span class="mf">0.2997</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0631</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1202</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0973</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">0.1269</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4766</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0509</span><span class="p">,</span>  <span class="mf">0.1725</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0470</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0562</span><span class="p">,</span>  <span class="mf">0.1357</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2274</span><span class="p">,</span>
          <span class="mf">0.3410</span><span class="p">],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">CatBackward</span><span class="o">&gt;</span><span class="p">)</span>
</code></pre></div></div> <h3 id="optimization-problem-circles-dataset">Optimization Problem: Circles Dataset</h3> <p>The optimization problem is the Circles dataset from Scikit-Learn, where the neural network model must learn to predict and discriminate between the inner circles (labelled 1) and outer circles (labelled 0). The Circles dataset is the reason that non-linear activation functions in the neural network architecture are needed. $X$ is 2-dimensional while $y$ is 1-dimensional.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_circles</span>

<span class="k">def</span> <span class="nf">plot_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
  <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">flatten</span><span class="p">()</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">0</span><span class="sh">'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">1</span><span class="sh">'</span><span class="p">)</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">X</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="nf">plot_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nf">net</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">5</span><span class="p">,</span> <span class="p">:])</span>

<span class="nf">tensor</span><span class="p">([[</span><span class="mf">0.5009</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.4840</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.6110</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.6143</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.5136</span><span class="p">]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">SigmoidBackward</span><span class="o">&gt;</span><span class="p">)</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/evo-comp/circles_dataset.png" data-zoomable=""/> </div> </div> <div class="caption"> The Circles dataset. </div> <h3 id="process-1-generate-the-initial-population-of-neural-networks">Process 1: Generate the initial population of neural networks.</h3> <p>For illustration purposes, a small population size of 5 and 4 hidden units per neural network layer is used. Inspecting the first 2 neural networks in the population, neural network weights are randomly initialised. The specific initialisation method used for the weights is documented in the <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">PyTorch documentation</a> for interested readers.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">population_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">initial_population</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="nc">Net</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">n_hidden</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">population_size</span><span class="p">)])</span>

<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">initial_population</span><span class="p">[:</span><span class="mi">2</span><span class="p">]:</span>
  <span class="nf">print</span><span class="p">(</span><span class="nf">get_params</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>

<span class="nf">tensor</span><span class="p">([</span> <span class="mf">0.5282</span><span class="p">,</span>  <span class="mf">0.4404</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6330</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1387</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2104</span><span class="p">,</span>  <span class="mf">0.5194</span><span class="p">,</span>  <span class="mf">0.3596</span><span class="p">,</span>  <span class="mf">0.2664</span><span class="p">,</span>
          <span class="mf">0.1044</span><span class="p">,</span>  <span class="mf">0.5380</span><span class="p">,</span>  <span class="mf">0.1583</span><span class="p">,</span>  <span class="mf">0.1221</span><span class="p">,</span>  <span class="mf">0.0324</span><span class="p">,</span>  <span class="mf">0.1239</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3698</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3658</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">0.0879</span><span class="p">],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">CatBackward</span><span class="o">&gt;</span><span class="p">)</span>
<span class="nf">tensor</span><span class="p">([</span> <span class="mf">0.3366</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6526</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1909</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4681</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3542</span><span class="p">,</span>  <span class="mf">0.2475</span><span class="p">,</span>  <span class="mf">0.1892</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1961</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">0.2867</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4570</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3183</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2220</span><span class="p">,</span>  <span class="mf">0.1677</span><span class="p">,</span>  <span class="mf">0.2118</span><span class="p">,</span>  <span class="mf">0.1053</span><span class="p">,</span>  <span class="mf">0.3238</span><span class="p">,</span>
          <span class="mf">0.0737</span><span class="p">],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">CatBackward</span><span class="o">&gt;</span><span class="p">)</span>
</code></pre></div></div> <h3 id="process-2-compute-the-fitness-of-the-population">Process 2: Compute the fitness of the population.</h3> <p>The fitness function measures the performance of an individual neural network. Because $y$ is a binary variable of values ${0,1}$, the negative binary cross entropy error (BCE) is employed, negated to reflect a higher value as more desirable.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">fitness_function</span><span class="p">(</span><span class="n">net</span><span class="p">):</span>
  <span class="k">return</span> <span class="o">-</span><span class="n">nn</span><span class="p">.</span><span class="nc">BCELoss</span><span class="p">()(</span><span class="nf">net</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">).</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">compute_fitness</span><span class="p">(</span><span class="n">population</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="nf">fitness_function</span><span class="p">(</span><span class="n">individual</span><span class="p">)</span> <span class="k">for</span> <span class="n">individual</span> <span class="ow">in</span> <span class="n">population</span><span class="p">])</span>

<span class="n">fitness_score</span> <span class="o">=</span> <span class="nf">fitness_function</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
<span class="n">fitness_scores</span> <span class="o">=</span> <span class="nf">compute_fitness</span><span class="p">(</span><span class="n">initial_population</span><span class="p">)</span>

<span class="n">fitness_score</span><span class="p">,</span> <span class="n">fitness_scores</span>


<span class="p">(</span><span class="o">-</span><span class="mf">0.7065134644508362</span><span class="p">,</span>
  <span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.69943392</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.70006615</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.70542192</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.69766504</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.76979303</span><span class="p">]))</span>
</code></pre></div></div> <h3 id="process-3-select-the-fittest-neural-networks">Process 3: Select the fittest neural networks.</h3> <p>Select the top $k$ percentage of neural networks with the highest fitness score to form the parent subpopulation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">solution</span><span class="p">(</span><span class="n">individual</span><span class="p">):</span>
  <span class="k">return</span> <span class="nf">individual</span><span class="p">(</span><span class="n">X</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">round</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">get_fittest</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">fitness_scores</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">population</span><span class="p">[</span><span class="n">fitness_scores</span><span class="p">.</span><span class="nf">argmax</span><span class="p">()]</span>

<span class="k">def</span> <span class="nf">select_fittest</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">fitness_scores</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">population</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">fitness_scores</span><span class="p">)[</span><span class="o">-</span><span class="nf">int</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">population</span><span class="p">)</span> <span class="o">*</span> <span class="n">k</span><span class="p">):]]</span>

<span class="n">parent_subpopulation</span> <span class="o">=</span> <span class="nf">select_fittest</span><span class="p">(</span><span class="n">initial_population</span><span class="p">,</span> <span class="n">fitness_scores</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="nf">compute_fitness</span><span class="p">(</span><span class="n">parent_subpopulation</span><span class="p">)</span>

<span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.69943392</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.69766504</span><span class="p">])</span>
</code></pre></div></div> <h3 id="process-4-perform-reproduction-of-the-parents-to-replenish-the-population">Process 4: Perform reproduction of the parents to replenish the population.</h3> <p>In contrast to common implementations of genetic algorithms, no crossing-over is performed. Parent neural networks are simply uniformly sampled with replacement to create an identical copy as the child.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">copy</span>

<span class="k">def</span> <span class="nf">perform_reproduction</span><span class="p">(</span><span class="n">subpopulation</span><span class="p">):</span>
  <span class="n">num_children</span> <span class="o">=</span> <span class="n">population_size</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">subpopulation</span><span class="p">)</span>
  <span class="n">parents</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">subpopulation</span><span class="p">,</span> <span class="n">num_children</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">subpopulation</span><span class="p">,</span> <span class="p">[</span><span class="n">copy</span><span class="p">.</span><span class="nf">deepcopy</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parents</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">next_population</span> <span class="o">=</span> <span class="nf">perform_reproduction</span><span class="p">(</span><span class="n">parent_subpopulation</span><span class="p">)</span>
<span class="nf">compute_fitness</span><span class="p">(</span><span class="n">next_population</span><span class="p">)</span>

<span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.69943392</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.69766504</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.69943392</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.69943392</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.69766504</span><span class="p">])</span>
</code></pre></div></div> <h3 id="process-5-perform-mutation-on-the-population-1">Process 5: Perform mutation on the population.</h3> <p>As explained previously, add a Gaussian noise perturbation to all parameters of the neural network.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_population_parameter</span><span class="p">(</span><span class="n">population</span><span class="p">):</span>
  <span class="k">return</span> <span class="p">[</span><span class="nf">get_params</span><span class="p">(</span><span class="n">net</span><span class="p">)</span> <span class="k">for</span> <span class="n">net</span> <span class="ow">in</span> <span class="n">population</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">perform_mutation</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">individual</span> <span class="ow">in</span> <span class="n">population</span><span class="p">:</span>
    <span class="nf">mutate_params</span><span class="p">(</span><span class="n">individual</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">population</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Before mutation:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">get_population_parameter</span><span class="p">(</span><span class="n">next_population</span><span class="p">))</span>

<span class="nf">perform_mutation</span><span class="p">(</span><span class="n">next_population</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">After mutation:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">get_population_parameter</span><span class="p">(</span><span class="n">next_population</span><span class="p">))</span>


<span class="n">Before</span> <span class="n">mutation</span><span class="p">:</span>
<span class="p">[</span><span class="nf">tensor</span><span class="p">([</span> <span class="mf">0.5282</span><span class="p">,</span>  <span class="mf">0.4404</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6330</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1387</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2104</span><span class="p">,</span>  <span class="mf">0.5194</span><span class="p">,</span>  <span class="mf">0.3596</span><span class="p">,</span>  <span class="mf">0.2664</span><span class="p">,</span>
          <span class="mf">0.1044</span><span class="p">,</span>  <span class="mf">0.5380</span><span class="p">,</span>  <span class="mf">0.1583</span><span class="p">,</span>  <span class="mf">0.1221</span><span class="p">,</span>  <span class="mf">0.0324</span><span class="p">,</span>  <span class="mf">0.1239</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3698</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3658</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">0.0879</span><span class="p">],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">CatBackward</span><span class="o">&gt;</span><span class="p">),</span> <span class="nf">tensor</span><span class="p">([</span> <span class="mf">0.3565</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1857</span><span class="p">,</span>  <span class="mf">0.2187</span><span class="p">,</span>  <span class="mf">0.1788</span><span class="p">,</span>  <span class="mf">0.1412</span><span class="p">,</span>  <span class="mf">0.1778</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6750</span><span class="p">,</span>  <span class="mf">0.6518</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">0.5023</span><span class="p">,</span>  <span class="mf">0.2402</span><span class="p">,</span>  <span class="mf">0.4160</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0343</span><span class="p">,</span>  <span class="mf">0.0818</span><span class="p">,</span>  <span class="mf">0.2978</span><span class="p">,</span>  <span class="mf">0.3582</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0858</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">0.3332</span><span class="p">],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">CatBackward</span><span class="o">&gt;</span><span class="p">),</span> <span class="nf">tensor</span><span class="p">([</span> <span class="mf">0.5282</span><span class="p">,</span>  <span class="mf">0.4404</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6330</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1387</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2104</span><span class="p">,</span>  <span class="mf">0.5194</span><span class="p">,</span>  <span class="mf">0.3596</span><span class="p">,</span>  <span class="mf">0.2664</span><span class="p">,</span>
          <span class="mf">0.1044</span><span class="p">,</span>  <span class="mf">0.5380</span><span class="p">,</span>  <span class="mf">0.1583</span><span class="p">,</span>  <span class="mf">0.1221</span><span class="p">,</span>  <span class="mf">0.0324</span><span class="p">,</span>  <span class="mf">0.1239</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3698</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3658</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">0.0879</span><span class="p">],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">CatBackward</span><span class="o">&gt;</span><span class="p">),</span> <span class="nf">tensor</span><span class="p">([</span> <span class="mf">0.5282</span><span class="p">,</span>  <span class="mf">0.4404</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6330</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1387</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2104</span><span class="p">,</span>  <span class="mf">0.5194</span><span class="p">,</span>  <span class="mf">0.3596</span><span class="p">,</span>  <span class="mf">0.2664</span><span class="p">,</span>
          <span class="mf">0.1044</span><span class="p">,</span>  <span class="mf">0.5380</span><span class="p">,</span>  <span class="mf">0.1583</span><span class="p">,</span>  <span class="mf">0.1221</span><span class="p">,</span>  <span class="mf">0.0324</span><span class="p">,</span>  <span class="mf">0.1239</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3698</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3658</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">0.0879</span><span class="p">],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">CatBackward</span><span class="o">&gt;</span><span class="p">),</span> <span class="nf">tensor</span><span class="p">([</span> <span class="mf">0.3565</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1857</span><span class="p">,</span>  <span class="mf">0.2187</span><span class="p">,</span>  <span class="mf">0.1788</span><span class="p">,</span>  <span class="mf">0.1412</span><span class="p">,</span>  <span class="mf">0.1778</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6750</span><span class="p">,</span>  <span class="mf">0.6518</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">0.5023</span><span class="p">,</span>  <span class="mf">0.2402</span><span class="p">,</span>  <span class="mf">0.4160</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0343</span><span class="p">,</span>  <span class="mf">0.0818</span><span class="p">,</span>  <span class="mf">0.2978</span><span class="p">,</span>  <span class="mf">0.3582</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0858</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">0.3332</span><span class="p">],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">CatBackward</span><span class="o">&gt;</span><span class="p">)]</span>

<span class="n">After</span> <span class="n">mutation</span><span class="p">:</span>
<span class="p">[</span><span class="nf">tensor</span><span class="p">([</span> <span class="mf">0.6775</span><span class="p">,</span>  <span class="mf">0.5253</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6425</span><span class="p">,</span>  <span class="mf">0.0200</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2527</span><span class="p">,</span>  <span class="mf">0.5128</span><span class="p">,</span>  <span class="mf">0.4704</span><span class="p">,</span>  <span class="mf">0.1674</span><span class="p">,</span>
          <span class="mf">0.0913</span><span class="p">,</span>  <span class="mf">0.3902</span><span class="p">,</span>  <span class="mf">0.1255</span><span class="p">,</span>  <span class="mf">0.2492</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1159</span><span class="p">,</span>  <span class="mf">0.0734</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2054</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3601</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">0.1615</span><span class="p">],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">CatBackward</span><span class="o">&gt;</span><span class="p">),</span> <span class="nf">tensor</span><span class="p">([</span> <span class="mf">0.4123</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2091</span><span class="p">,</span>  <span class="mf">0.3269</span><span class="p">,</span>  <span class="mf">0.2261</span><span class="p">,</span>  <span class="mf">0.2042</span><span class="p">,</span>  <span class="mf">0.2701</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7392</span><span class="p">,</span>  <span class="mf">0.5924</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">0.5903</span><span class="p">,</span>  <span class="mf">0.0912</span><span class="p">,</span>  <span class="mf">0.2945</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2038</span><span class="p">,</span>  <span class="mf">0.0711</span><span class="p">,</span>  <span class="mf">0.1820</span><span class="p">,</span>  <span class="mf">0.3112</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0067</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">0.3152</span><span class="p">],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">CatBackward</span><span class="o">&gt;</span><span class="p">),</span> <span class="nf">tensor</span><span class="p">([</span> <span class="mf">0.6215</span><span class="p">,</span>  <span class="mf">0.5592</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5993</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0947</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3291</span><span class="p">,</span>  <span class="mf">0.4888</span><span class="p">,</span>  <span class="mf">0.3596</span><span class="p">,</span>  <span class="mf">0.2436</span><span class="p">,</span>
          <span class="mf">0.2440</span><span class="p">,</span>  <span class="mf">0.4675</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0707</span><span class="p">,</span>  <span class="mf">0.0269</span><span class="p">,</span>  <span class="mf">0.0515</span><span class="p">,</span>  <span class="mf">0.0444</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3241</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3425</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">0.1050</span><span class="p">],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">CatBackward</span><span class="o">&gt;</span><span class="p">),</span> <span class="nf">tensor</span><span class="p">([</span> <span class="mf">0.4444</span><span class="p">,</span>  <span class="mf">0.4725</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6944</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2502</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1153</span><span class="p">,</span>  <span class="mf">0.5679</span><span class="p">,</span>  <span class="mf">0.2696</span><span class="p">,</span>  <span class="mf">0.4509</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">0.0502</span><span class="p">,</span>  <span class="mf">0.6541</span><span class="p">,</span>  <span class="mf">0.0673</span><span class="p">,</span>  <span class="mf">0.1718</span><span class="p">,</span>  <span class="mf">0.0901</span><span class="p">,</span>  <span class="mf">0.0092</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2689</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4209</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">0.0223</span><span class="p">],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">CatBackward</span><span class="o">&gt;</span><span class="p">),</span> <span class="nf">tensor</span><span class="p">([</span> <span class="mf">0.2158</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1765</span><span class="p">,</span>  <span class="mf">0.0658</span><span class="p">,</span>  <span class="mf">0.1894</span><span class="p">,</span>  <span class="mf">0.0741</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1204</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7014</span><span class="p">,</span>  <span class="mf">0.5762</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">0.3188</span><span class="p">,</span>  <span class="mf">0.3198</span><span class="p">,</span>  <span class="mf">0.5875</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0955</span><span class="p">,</span>  <span class="mf">0.0913</span><span class="p">,</span>  <span class="mf">0.2711</span><span class="p">,</span>  <span class="mf">0.3587</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0755</span><span class="p">,</span>
        <span class="o">-</span><span class="mf">0.4120</span><span class="p">],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">CatBackward</span><span class="o">&gt;</span><span class="p">)]</span>
</code></pre></div></div> <h3 id="the-neuroevolution-algorithm-all-5-processes-together">The Neuroevolution Algorithm: All 5 Processes Together</h3> <p>By combining the 5 processes together, we construct the neuroevolution algorithm and run it to find a neural network solution that models the Circles dataset well.</p> <p>Neuroevolution:</p> <ol> <li>Generate the initial population of individuals.</li> <li>Repeat until convergence: <ol> <li>Compute fitness of the population.</li> <li>Select the fittest individuals (parent subpopulation).</li> <li>Perform reproduction between parents to produce children.</li> <li>Perform mutation on the population.</li> </ol> </li> <li>Select the fittest individual of the population as the solution.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Neuroevolution hyperparameters
</span><span class="n">population_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">num_generations</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">top_k</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">mutation_sigma</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">16</span>

<span class="c1"># Process 1: Generate the initial population.
</span><span class="n">population</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="nc">Net</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">population_size</span><span class="p">)])</span>

<span class="c1"># Misc: Experimental tracking
</span><span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">solutions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">fittests</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_generations</span><span class="p">):</span>
  <span class="c1"># Process 2: Compute fitness of the population.
</span>  <span class="n">fitness_scores</span> <span class="o">=</span> <span class="nf">compute_fitness</span><span class="p">(</span><span class="n">population</span><span class="p">)</span>

  <span class="c1"># Process 3: Select the fittest individuals.
</span>  <span class="n">fittest_subpopulation</span> <span class="o">=</span> <span class="nf">select_fittest</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">fitness_scores</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">top_k</span><span class="p">)</span>
  
  <span class="c1"># Misc: Experimental tracking
</span>  <span class="n">fittest</span> <span class="o">=</span> <span class="nf">get_fittest</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">fitness_scores</span><span class="p">)</span>
  <span class="n">fittests</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">fittest</span><span class="p">)</span>
  <span class="n">solutions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">solution</span><span class="p">(</span><span class="n">fittest</span><span class="p">))</span>
  <span class="n">scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">fitness_function</span><span class="p">(</span><span class="n">fittest</span><span class="p">))</span>

  <span class="c1"># Process 4: Perform reproduction between parents.
</span>  <span class="n">children</span> <span class="o">=</span> <span class="nf">perform_reproduction</span><span class="p">(</span><span class="n">fittest_subpopulation</span><span class="p">)</span>

  <span class="c1"># Process 5: Perform mutation on the population.
</span>  <span class="n">population</span> <span class="o">=</span> <span class="nf">perform_mutation</span><span class="p">(</span><span class="n">children</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">mutation_sigma</span><span class="p">)</span>


<span class="c1"># Misc: Experimental tracking
</span><span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">num_generations</span><span class="p">),</span> <span class="n">scores</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/evo-comp/ne_training_plot.png" data-zoomable=""/> </div> </div> <div class="caption"> Score over generations. </div> <h3 id="experiment-result-1">Experiment Result</h3> <p>The background colours illustrate the neural network’s decision boundary, while the individual data points are the original dataset. Looking at the fittest individual neural network of the final population, the non-linear decision boundary has been correctly and well-learnt by the fittest neural network in the final population.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_individual</span><span class="p">(</span><span class="n">net</span><span class="p">):</span>
  <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">].</span><span class="nf">min</span><span class="p">()</span><span class="o">*</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">].</span><span class="nf">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
  <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">].</span><span class="nf">min</span><span class="p">()</span><span class="o">*</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">].</span><span class="nf">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
  <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>

  <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">X1</span><span class="p">.</span><span class="n">shape</span><span class="p">).</span><span class="nf">flatten</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">[</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">]</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">X1</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(),</span> <span class="n">X2</span><span class="p">.</span><span class="nf">flatten</span><span class="p">())):</span>
    <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="nf">net</span><span class="p">(</span><span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">])).</span><span class="nf">float</span><span class="p">()).</span><span class="n">data</span><span class="p">)</span>
  <span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">X1</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

  <span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="mf">1.2</span><span class="p">,</span> <span class="nf">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="mf">1.2</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span><span class="o">*</span><span class="mf">1.2</span><span class="p">,</span> <span class="nf">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span><span class="o">*</span><span class="mf">1.2</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">contourf</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">bwr</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">()</span>


<span class="n">fitness_score</span> <span class="o">=</span> <span class="nf">fitness_function</span><span class="p">(</span><span class="n">fittest</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Fittest score: </span><span class="si">{</span><span class="n">fitness_score</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">plot_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nf">plot_individual</span><span class="p">(</span><span class="n">fittest</span><span class="p">)</span>


<span class="n">Fittest</span> <span class="n">score</span><span class="p">:</span> <span class="o">-</span><span class="mf">0.028863554820418358</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/evo-comp/decision_boundary.png" data-zoomable=""/> </div> </div> <div class="caption"> Fitted decision boundary. </div> <p>By visualising the fittest model at each generation of neuroevolution, notice that the circular decision boundary is eventually found. For an evolutionary strategy based on novelty applied on reinforcement learning, refer to <a href="https://jetnew.io/posts/2020/11/novelty-search/">Part 3</a> of the Evolutionary Computation series on Novelty Search. For an introductory treatment of the genetic algorithm, refer to <a href="https://jetnew.io/posts/2020/11/genetic-algorithm/">Part 1</a>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">capture</span>
<span class="kn">from</span> <span class="n">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">()</span>

<span class="nf">plot_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mf">1.2</span><span class="p">),</span> <span class="nf">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="mf">1.2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mf">1.2</span><span class="p">),</span> <span class="nf">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span><span class="o">*</span><span class="mf">1.2</span><span class="p">)</span>

<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">].</span><span class="nf">min</span><span class="p">()</span><span class="o">*</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">].</span><span class="nf">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">].</span><span class="nf">min</span><span class="p">()</span><span class="o">*</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">].</span><span class="nf">max</span><span class="p">()</span><span class="o">*</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">animate</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
  <span class="n">net</span> <span class="o">=</span> <span class="n">fittests</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
  <span class="n">Y</span> <span class="o">=</span> <span class="nf">net</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">Tensor</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">X1</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(),</span> <span class="n">X2</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))).</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">reshape</span><span class="p">(</span><span class="n">X1</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="nf">contourf</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">bwr</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Gen </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="n">ani</span> <span class="o">=</span> <span class="nc">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">animate</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_generations</span><span class="p">),</span> <span class="n">interval</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="sh">'</span><span class="s">/images/neuroevolution/neuroevolution.gif</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/evo-comp/neuroevolution.gif" data-zoomable=""/> </div> </div> <div class="caption"> Neuroevolution on Circles dataset over generations. </div> <hr/> <h2 id="novelty-search">Novelty Search</h2> <p>Novelty Search is an Evolutionary Strategy (ES) algorithm that optimises using a novelty function instead of a fitness function (like in a vanilla genetic algorithm), which has shown to produce competitive performance for exploration in reinforcement learning. The novelty of a solution is defined by how similar the solution’s behaviour is as compared to the rest of the population. The novelty score is therefore computed by its average distance from the $k$-nearest neighbours in the population.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">gym</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="n">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
</code></pre></div></div> <h3 id="neuroevolution-for-reinforcement-learning">Neuroevolution for Reinforcement Learning</h3> <p>Neuroevolution is the application of evolutionary strategies to neural networks. We use a simple neural network in PyTorch, with 2 linear layers and 2 non-linear activation functions tangent and sigmoid. In deep reinforcement learning, the neural network policy $\pi: s \rightarrow a$ serves as function mapping from the observation of the environment to an action chosen by the agent. Over one episode, the agent performs an action and the state of the environment is observed by the agent, along with a reward at that particular timestep. The fitness of an individual neural network is therefore defined by the cumulative reward obtained by the agent over one episode of interacting with the environment. The environment used is <a href="https://gym.openai.com/envs/CartPole-v1/">CartPole-v1</a>, where the agent’s goal is to balance the pole by pushing the cart. The state observed by the agent is defined as:</p> \[Observation = [Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity]\] <p>where the range of values are:</p> \[Cart Position = [-4.8,4.8]\] \[Cart Velocity = [-Inf, Inf]\] \[Pole Angle = [-24 degrees, 24 degrees]\] \[Pole Angular Velocity = [-Inf, Inf]\] <p>and the action is a single scalar discrete value:</p> \[Action = [0, 1]\] <p>where $0$ and $1$ represents the action of pushing the cart to the left and right respectively.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">tanh1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sigmoid</span><span class="p">()</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">tanh1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">obs</span><span class="p">):</span>
  <span class="k">return</span> <span class="nf">net</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">obs</span><span class="p">.</span><span class="nf">copy</span><span class="p">()).</span><span class="nf">float</span><span class="p">()).</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">argmax</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">net</span><span class="p">):</span>
  <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
  <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
  <span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">obs</span><span class="p">)</span>
    <span class="n">obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>
  <span class="k">return</span> <span class="n">total_reward</span>

<span class="k">def</span> <span class="nf">fitness_function</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">([</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">net</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">)])</span>

<span class="k">def</span> <span class="nf">compute_fitness</span><span class="p">(</span><span class="n">population</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="nf">fitness_function</span><span class="p">(</span><span class="n">individual</span><span class="p">)</span> <span class="k">for</span> <span class="n">individual</span> <span class="ow">in</span> <span class="n">population</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">get_fittest</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">fitness_scores</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">population</span><span class="p">[</span><span class="n">fitness_scores</span><span class="p">.</span><span class="nf">argmax</span><span class="p">()]</span>
</code></pre></div></div> <h3 id="novelty-selection">Novelty Selection</h3> <p>The main difference between neuroevolution and novelty search is the selection criterion, changed from the fitness score to the novelty score. Instead of selecting for the fittest individuals in a population, novelty search selects the most novel individuals by the novelty score with respect to the rest of the population. The novelty score indicates the novelty of an individual, defined as the average difference of an individual neural network policy $\pi$ to $k$-nearest neighbours in the population, notably in terms of their behaviour. Therefore, the behaviour of an individual $b(\pi_i)$ must be defined. We employ a simple characterisation of a neural network’s behaviour as the terminal (final) state $s_n$ in the sequence of states observed by the agent $S_{\pi_i} = [s_1, s_2, …, s_n]$ in 1 evaluation:</p> \[Behaviour(\pi_i) = Terminal(S_{\pi_i}) = s_{n}\] <p>The similarity between 2 individuals’ behaviours is simply the sum of squared difference between final observations:</p> \[Similarity(\pi_i, \pi_j) = \Vert Behaviour(\pi_i) - Behaviour(\pi_j)\Vert_2\] <p>The novelty of an individual with respect to its $k$-nearest neighbours of the population $P$ is defined by:</p> \[Novelty(\pi_i, N_{\pi_i}) = \frac{1}{|N_{\pi_i}|} \sum_{\pi_k\in N_{\pi_i}}Similarity(\pi_i, \pi_k)\] <p>where $N_{\pi_i}$ refers to the $k$-nearest neighbours of $\pi_i$. The $k$-nearest neighbours $N_{\pi_i}$ are selected by the $k$ largest similarity scores between $\pi_i$ and $\pi_{k}\in P$.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/evo-comp/novelty_knn.jpg" data-zoomable=""/> </div> </div> <div class="caption"> Local sparseness around each individual behaviour, (<a href="https://dl.acm.org/doi/10.1016/j.ins.2016.06.044">Naredo 2016</a>) </div> <p>As shown above, individuals in a dense region of the behaviour space have less novel behaviour, while individuals in a sparse region have most novel behaviour and are selected for.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">behaviour</span><span class="p">(</span><span class="n">net</span><span class="p">):</span>
  <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
  <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
  <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">obs</span><span class="p">)</span>
    <span class="n">obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">obs</span>

<span class="k">def</span> <span class="nf">similarity</span><span class="p">(</span><span class="n">net1</span><span class="p">,</span> <span class="n">net2</span><span class="p">):</span>
    <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="nf">behaviour</span><span class="p">(</span><span class="n">net1</span><span class="p">),</span> <span class="nf">behaviour</span><span class="p">(</span><span class="n">net2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">b1</span> <span class="o">-</span> <span class="n">b2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_novelty</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">distances</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">population</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">distance_i</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">([</span><span class="nf">similarity</span><span class="p">(</span><span class="n">population</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">population</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">j</span><span class="p">])[:</span><span class="n">k</span><span class="p">]</span>
        <span class="n">distances</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">distance_i</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">distances</span>

<span class="k">def</span> <span class="nf">get_novel_subpopulation</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">novelty_scores</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">population</span><span class="p">[</span><span class="n">novelty_scores</span><span class="p">.</span><span class="nf">argmax</span><span class="p">()]</span>

<span class="k">def</span> <span class="nf">select_most_novel</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">novelty_scores</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">population</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">novelty_scores</span><span class="p">)[</span><span class="o">-</span><span class="nf">int</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">population</span><span class="p">)</span> <span class="o">*</span> <span class="n">k</span><span class="p">):]]</span>
</code></pre></div></div> <h3 id="perform-reproduction">Perform Reproduction</h3> <p>As with neuroevolution, reproduction among the novel parent individuals is performed by simply sampling then making a copy of the parent individual to form child individuals, replenishing the population. There is no change from the neuroevolution implementation the above section on neuroevolution.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">copy</span>

<span class="k">def</span> <span class="nf">perform_reproduction</span><span class="p">(</span><span class="n">subpopulation</span><span class="p">):</span>
  <span class="n">num_children</span> <span class="o">=</span> <span class="n">population_size</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">subpopulation</span><span class="p">)</span>
  <span class="n">parents</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">subpopulation</span><span class="p">,</span> <span class="n">num_children</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">subpopulation</span><span class="p">,</span> <span class="p">[</span><span class="n">copy</span><span class="p">.</span><span class="nf">deepcopy</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">parents</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div> <h3 id="perform-mutation">Perform Mutation</h3> <p>As with neuroevolution, mutation is performed by applying an additive Gaussian noise to the parameters of the neural networks. There is no change from the neuroevolution implementation in <a href="https://jetnew.io/posts/2020/11/neuroevolution/">Part 2</a>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torch.nn.utils</span> <span class="kn">import</span> <span class="n">parameters_to_vector</span><span class="p">,</span> <span class="n">vector_to_parameters</span>

<span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="n">net</span><span class="p">):</span>
  <span class="k">return</span> <span class="nf">parameters_to_vector</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>

<span class="k">def</span> <span class="nf">mutate_params</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="n">mutated_params</span> <span class="o">=</span> <span class="nf">get_params</span><span class="p">(</span><span class="n">net</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nf">get_params</span><span class="p">(</span><span class="n">net</span><span class="p">).</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nf">vector_to_parameters</span><span class="p">(</span><span class="n">mutated_params</span><span class="p">,</span> <span class="n">net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>

<span class="k">def</span> <span class="nf">perform_mutation</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">individual</span> <span class="ow">in</span> <span class="n">population</span><span class="p">:</span>
    <span class="nf">mutate_params</span><span class="p">(</span><span class="n">individual</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">population</span>
</code></pre></div></div> <h3 id="the-novelty-search-algorithm">The Novelty Search Algorithm</h3> <p>By selecting the most novel individuals over generations, the individuals in the population will find its behavioural niche, improving exploration in the behaviour space.</p> <p>Novelty Search:</p> <ol> <li>Generate the initial population of individuals.</li> <li>Repeat until convergence: <ol> <li>Compute novelty of the population.</li> <li>Select the most novel individuals to form the parent subpopulation.</li> <li>Perform reproduction between parents to produce children.</li> <li>Perform mutation on the population.</li> </ol> </li> <li>Select the fittest individual of the population as the solution.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Novelty Search hyperparameters
</span><span class="n">population_size</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">num_generations</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">top_k</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">mutation_sigma</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">k_nearest</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># CartPole environment initialisation
</span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">CartPole-v1</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Neural network hyperparameters
</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">output_size</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span>
<span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">16</span>

<span class="c1"># Process 1: Generate the initial population.
</span><span class="n">population</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="nc">Net</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">population_size</span><span class="p">)])</span>

<span class="c1"># Misc: Experimental tracking
</span><span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">fittests</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_generations</span><span class="p">):</span>

  <span class="c1"># Process 2: Compute the novelty of individuals with respect to closest neighbours in the population.
</span>  <span class="n">novelty_scores</span> <span class="o">=</span> <span class="nf">compute_novelty</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k_nearest</span><span class="p">)</span>
    
  <span class="c1"># Process 3: Select the most novel individuals.
</span>  <span class="n">novel_subpopulation</span> <span class="o">=</span> <span class="nf">select_most_novel</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">novelty_scores</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">top_k</span><span class="p">)</span>

  <span class="c1"># Misc: Experimental tracking
</span>  <span class="n">fitness_scores</span> <span class="o">=</span> <span class="nf">compute_fitness</span><span class="p">(</span><span class="n">population</span><span class="p">)</span>
  <span class="n">fittest</span> <span class="o">=</span> <span class="nf">get_fittest</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">fitness_scores</span><span class="p">)</span>
  <span class="n">fittests</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">fittest</span><span class="p">)</span>
  <span class="n">scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">max</span><span class="p">(</span><span class="n">fitness_scores</span><span class="p">))</span>

  <span class="c1"># Process 4: Perform reproduction between parents.
</span>  <span class="n">children</span> <span class="o">=</span> <span class="nf">perform_reproduction</span><span class="p">(</span><span class="n">novel_subpopulation</span><span class="p">)</span>
  <span class="n">population</span> <span class="o">=</span> <span class="nf">perform_mutation</span><span class="p">(</span><span class="n">children</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">mutation_sigma</span><span class="p">)</span>


<span class="c1"># Misc: Experimental tracking
</span><span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">num_generations</span><span class="p">),</span> <span class="n">scores</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/evo-comp/ns_training_plot.png" data-zoomable=""/> </div> </div> <div class="caption"> Score over generations. </div> <h3 id="experiment-results">Experiment Results</h3> <p>Plotting the novelty score against fitness score for the final population, the novelty score defined by the $k$-nearest neighbour similarity of terminal states is not linearly correlated with a high fitness score. It is important that the novelty score is not linearly correlated with the fitness score because a linear combination of the fitness score would have been used for selection, defeating the purpose of novelty-based search.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Novelty Score</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Fitness Score</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">novelty_scores</span><span class="p">,</span> <span class="n">fitness_scores</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/evo-comp/novelty-fitness.png" data-zoomable=""/> </div> </div> <div class="caption"> Novelty (x-axis) is not linearly correlated with the fitness score (y-axis). </div> <p>Visualising an episode of the fittest individual shows that the novelty search algorithm has successfully achieved the goal of CartPole-v1 of balancing the pole.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">capture</span>
<span class="kn">from</span> <span class="n">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span>

<span class="k">def</span> <span class="nf">get_frames</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
  <span class="n">frames</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
      <span class="n">action</span> <span class="o">=</span> <span class="nf">get_action</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">obs</span><span class="p">)</span>
      <span class="n">obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
      <span class="n">frames</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="nf">render</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">rgb_array</span><span class="sh">'</span><span class="p">))</span>
  <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">frames</span>

<span class="n">frames</span> <span class="o">=</span> <span class="nf">get_frames</span><span class="p">(</span><span class="n">fittest</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">()</span>
<span class="n">screen</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">frames</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">animate</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
  <span class="n">screen</span><span class="p">.</span><span class="nf">set_data</span><span class="p">(</span><span class="n">frames</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

<span class="n">ani</span> <span class="o">=</span> <span class="nc">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">animate</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">frames</span><span class="p">)),</span> <span class="n">interval</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="sh">'</span><span class="s">/images/novelty-search/novelty_search.gif</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/evo-comp/novelty_search.gif" data-zoomable=""/> </div> </div> <div class="caption"> Fitted solution trained by Novelty Search on the cartpole gym. </div>]]></content><author><name></name></author><category term="tutorial"/><category term="research"/><summary type="html"><![CDATA[A blog post on evolutionary computation, a family of optimization algorithms inspired by biological evolution.]]></summary></entry><entry><title type="html">Gaussian Mixture Models with TensorFlow Probability</title><link href="/blog/2020/gmm-tfp/" rel="alternate" type="text/html" title="Gaussian Mixture Models with TensorFlow Probability"/><published>2020-06-27T00:00:00+00:00</published><updated>2020-06-27T00:00:00+00:00</updated><id>/blog/2020/gmm-tfp</id><content type="html" xml:base="/blog/2020/gmm-tfp/"><![CDATA[<h2 id="content">Content</h2> <ol> <li>Statistics</li> <li>Gaussian</li> <li>Multivariate Gaussian</li> <li>Gaussian Mixture Model</li> <li>Multivariate Gaussian Mixture Model</li> <li>Conditional Gaussian Mixture Model</li> </ol> <p>In probability theory, a normal (or Gaussian or Gauss or Laplace–Gauss) distribution is a type of continuous probability distribution for a real-valued random variable. — Wikipedia</p> <h2 id="dependencies">Dependencies</h2> <p>The required dependencies are Python 3.8, Numpy, Pandas, Matplotlib, TensorFlow, and Tensorflow-Probability.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">mpl_toolkits</span> <span class="kn">import</span> <span class="n">mplot3d</span>
<span class="kn">import</span> <span class="n">scipy</span>
<span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="n">tensorflow_probability</span> <span class="k">as</span> <span class="n">tfp</span>
<span class="n">tfd</span> <span class="o">=</span> <span class="n">tfp</span><span class="p">.</span><span class="n">distributions</span>
</code></pre></div></div> <h2 id="statistics">Statistics</h2> <p>The statistics required are: mean, covariance, diagonal, and standard deviation. We first generate X, a 2D array, then use the Numpy methods to compare statistics against the parameters used.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># random seed
</span><span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">X_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X_cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">cov</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X_diag</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">diag</span><span class="p">(</span><span class="n">X_cov</span><span class="p">)</span>
<span class="n">X_stddev</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">X_diag</span><span class="p">)</span>
<span class="c1"># X_mean
</span><span class="p">[</span><span class="o">-</span><span class="mf">9.57681805e-04</span>  <span class="mf">1.14277867e+00</span><span class="p">]</span>
<span class="c1"># X_cov
</span><span class="p">[[</span> <span class="mf">1.05494742</span> <span class="o">-</span><span class="mf">0.02517201</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.02517201</span>  <span class="mf">1.04230397</span><span class="p">]]</span>
<span class="c1"># X_diag
</span><span class="p">[</span><span class="mf">1.05494742</span> <span class="mf">1.04230397</span><span class="p">]</span>
<span class="c1"># X_stddev
</span><span class="p">[</span><span class="mf">1.02710633</span> <span class="mf">1.02093289</span><span class="p">]</span>
</code></pre></div></div> <p>Notice that the values of mean and covariance computed from <code class="language-plaintext highlighter-rouge">X</code> are comparable to the parameters specified to generate <code class="language-plaintext highlighter-rouge">X</code>. <code class="language-plaintext highlighter-rouge">np.cov</code> uses the parameter <code class="language-plaintext highlighter-rouge">rowvar=0</code> to convert rows of samples into rows of variables to compute the covariance matrix. <code class="language-plaintext highlighter-rouge">np.diag</code> obtains the diagonal, which is the variances from a covariance matrix. <code class="language-plaintext highlighter-rouge">np.sqrt</code> will obtain the standard deviations of the diagonal.</p> <h2 id="gaussian">Gaussian</h2> <p>The Gaussian distribution is defined by its probability density function:</p> \[p(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}\] <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/gmm-tfp/gaussian-pdf.png" data-zoomable=""/> </div> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/gmm-tfp/gaussian-cdf.png" data-zoomable=""/> </div> </div> <div class="caption"> Gaussian probability density function (left) and cumulative density function (right). </div> <h2 id="multivariate-gaussian">Multivariate Gaussian</h2> <p>The multivariate Gaussian can be modelled using <code class="language-plaintext highlighter-rouge">tfd.MultivariateNormalFullCovariance</code>, parameterised by <code class="language-plaintext highlighter-rouge">loc</code> and <code class="language-plaintext highlighter-rouge">covariance_matrix</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mvn</span> <span class="o">=</span> <span class="n">tfd</span><span class="p">.</span><span class="nc">MultivariateNormalFullCovariance</span><span class="p">(</span>
 <span class="n">loc</span><span class="o">=</span><span class="n">X_mean</span><span class="p">,</span>
 <span class="n">covariance_matrix</span><span class="o">=</span><span class="n">X_cov</span><span class="p">)</span>
<span class="n">mvn_mean</span> <span class="o">=</span> <span class="n">mvn</span><span class="p">.</span><span class="nf">mean</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">mvn_cov</span> <span class="o">=</span> <span class="n">mvn</span><span class="p">.</span><span class="nf">covariance</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">mvn_stddev</span> <span class="o">=</span> <span class="n">mvn</span><span class="p">.</span><span class="nf">stddev</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
<span class="c1"># mvn_mean
</span><span class="p">[</span><span class="o">-</span><span class="mf">0.00135437</span>  <span class="mf">1.20191953</span><span class="p">]</span>
<span class="c1"># mvn_cov
</span><span class="p">[[</span> <span class="mf">2.10989483</span> <span class="o">-</span><span class="mf">0.05034403</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.05034403</span>  <span class="mf">2.08460795</span><span class="p">]]</span>
<span class="c1"># mvn_stddev
</span><span class="p">[</span><span class="mf">1.4525477</span>  <span class="mf">1.44381714</span><span class="p">]</span>
</code></pre></div></div> <p>However, <code class="language-plaintext highlighter-rouge">tfd.MultivariateNormalFullCovariance</code> will be deprecated and <code class="language-plaintext highlighter-rouge">MultivariateNormalTril(loc=loc, scale_tril=tf.linalg.cholesky(covariance_matrix))</code> should be used instead. Cholesky decomposition of a positive definite matrix (e.g. covariance matrix) can be interpreted as the “square root” of a positive definite matrix [<a href="http://www.seas.ucla.edu/~vandenbe/133A/lectures/chol.pdf">1</a>][<a href="http://ais.informatik.uni-freiburg.de/teaching/ws12/mapping/pdf/slam05-ukf.pdf">2</a>].</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Due to deprecated MultivariateNormalFullCovariance
</span><span class="n">mvn</span> <span class="o">=</span> <span class="n">tfd</span><span class="p">.</span><span class="nc">MultivariateNormalTriL</span><span class="p">(</span>
 <span class="n">loc</span><span class="o">=</span><span class="n">X_mean</span><span class="p">,</span>
 <span class="n">scale_tril</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">cholesky</span><span class="p">(</span><span class="n">X_cov</span><span class="p">))</span>
<span class="n">mvn_mean</span> <span class="o">=</span> <span class="n">mvn</span><span class="p">.</span><span class="nf">mean</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">mvn_cov</span> <span class="o">=</span> <span class="n">mvn</span><span class="p">.</span><span class="nf">covariance</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">mvn_stddev</span> <span class="o">=</span> <span class="n">mvn</span><span class="p">.</span><span class="nf">stddev</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
<span class="c1"># mvn_mean
</span><span class="p">[</span><span class="o">-</span><span class="mf">0.00135437</span>  <span class="mf">1.20191953</span><span class="p">]</span>
<span class="c1"># mvn_cov
</span><span class="p">[[</span> <span class="mf">2.10989483</span> <span class="o">-</span><span class="mf">0.05034403</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.05034403</span>  <span class="mf">2.08460795</span><span class="p">]]</span>
<span class="c1"># mvn_stddev
</span><span class="p">[</span><span class="mf">1.4525477</span>  <span class="mf">1.44381714</span><span class="p">]</span>
</code></pre></div></div> <p>Instead of specifying the covariance matrix, the standard deviation can be specified for <code class="language-plaintext highlighter-rouge">tfd.MultivariateNormalDiag</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mvn</span> <span class="o">=</span> <span class="n">tfd</span><span class="p">.</span><span class="nc">MultivariateNormalDiag</span><span class="p">(</span>
 <span class="n">loc</span><span class="o">=</span><span class="n">X_mean</span><span class="p">,</span>
 <span class="n">scale_diag</span><span class="o">=</span><span class="n">X_stddev</span><span class="p">)</span>
<span class="n">mvn_mean</span> <span class="o">=</span> <span class="n">mvn</span><span class="p">.</span><span class="nf">mean</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">mvn_cov</span> <span class="o">=</span> <span class="n">mvn</span><span class="p">.</span><span class="nf">covariance</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">mvn_stddev</span> <span class="o">=</span> <span class="n">mvn</span><span class="p">.</span><span class="nf">stddev</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
<span class="c1"># mvn_mean
</span><span class="p">[</span><span class="o">-</span><span class="mf">0.00135437</span>  <span class="mf">1.20191953</span><span class="p">]</span>
<span class="c1"># mvn_cov
</span><span class="p">[[</span><span class="mf">2.10989483</span> <span class="mf">0.</span>        <span class="p">]</span>
 <span class="p">[</span><span class="mf">0.</span>         <span class="mf">2.08460795</span><span class="p">]]</span>
<span class="c1"># mvn_stddev
</span><span class="p">[</span><span class="mf">1.4525477</span>  <span class="mf">1.44381714</span><span class="p">]</span>
</code></pre></div></div> <p>To visualise the probability density function for the multivariate Gaussian, <code class="language-plaintext highlighter-rouge">plt.contour</code> can be used.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">((</span><span class="n">x1</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(),</span> <span class="n">x2</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">prob</span> <span class="o">=</span> <span class="n">mvn</span><span class="p">.</span><span class="nf">prob</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">axes</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="sh">'</span><span class="s">3d</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot_surface</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">prob</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">x1</span><span class="p">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">viridis</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/gmm-tfp/multivariate-gaussian-pdf.png" data-zoomable=""/> </div> </div> <div class="caption"> Multivariate Gaussian probability density function. </div> <h2 id="gaussian-mixture-model">Gaussian Mixture Model</h2> <p>The Gaussian mixture model (GMM) is a mixture of Gaussians, each parameterised by by $\mu_k$ and $\sigma_k$, and linearly combined with each component weight, $\theta_k$, that sum to 1. The GMM can be defined by its probability density function:</p> \[p(x) = \sum_{k=1}^K \theta_k\cdot N(x\vert\mu,\sigma)\] <p>Take a mixture of Gaussians parameterised by <code class="language-plaintext highlighter-rouge">pi=[0.2,0.3,0.5]</code>, <code class="language-plaintext highlighter-rouge">mu=[10,20,30]</code>, and <code class="language-plaintext highlighter-rouge">sigma=[1,2,3]</code>. A categorical distribution <code class="language-plaintext highlighter-rouge">tfd.Categorical(probs=pi)</code> is a discrete probability distribution that models a random variable that takes 1 of <code class="language-plaintext highlighter-rouge">K</code> possible categories.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">gmm</span> <span class="o">=</span> <span class="n">tfd</span><span class="p">.</span><span class="nc">Mixture</span><span class="p">(</span>
    <span class="n">cat</span><span class="o">=</span><span class="n">tfd</span><span class="p">.</span><span class="nc">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">pi</span><span class="p">),</span>
    <span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="n">tfd</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">m</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)]</span>
<span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gmm</span><span class="p">.</span><span class="nf">prob</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">numpy</span><span class="p">());</span>
<span class="nf">print</span><span class="p">(</span><span class="n">gmm</span><span class="p">.</span><span class="nf">mean</span><span class="p">().</span><span class="nf">numpy</span><span class="p">())</span>  <span class="c1"># 23.0
</span></code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/gmm-tfp/gmm-pdf.png" data-zoomable=""/> </div> </div> <div class="caption"> Gaussian mixture model probability density function. </div> <p><code class="language-plaintext highlighter-rouge">tfd.MixtureSameFamily</code> allows definition of mixture models of the same family distribution without a for-loop.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gmm</span> <span class="o">=</span> <span class="n">tfd</span><span class="p">.</span><span class="nc">MixtureSameFamily</span><span class="p">(</span>
    <span class="n">mixture_distribution</span><span class="o">=</span><span class="n">tfd</span><span class="p">.</span><span class="nc">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">pi</span><span class="p">),</span>
    <span class="n">components_distribution</span><span class="o">=</span><span class="n">tfd</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">gmm</span><span class="p">.</span><span class="nf">mean</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>  <span class="c1"># 23.0
</span></code></pre></div></div> <h2 id="multivariate-gaussian-mixture-model">Multivariate Gaussian Mixture Model</h2> <p>Multivariate Gaussian mixture models can be implemented using TensorFlow-Probability by combining <code class="language-plaintext highlighter-rouge">tfd.MixtureSameFamily</code> with <code class="language-plaintext highlighter-rouge">tfd.MultivariateNormalDiag</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
               <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
               <span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">mvgmm</span> <span class="o">=</span> <span class="n">tfd</span><span class="p">.</span><span class="nc">MixtureSameFamily</span><span class="p">(</span>
    <span class="n">mixture_distribution</span><span class="o">=</span><span class="n">tfd</span><span class="p">.</span><span class="nc">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">pi</span><span class="p">),</span>
    <span class="n">components_distribution</span><span class="o">=</span><span class="n">tfd</span><span class="p">.</span><span class="nc">MultivariateNormalDiag</span><span class="p">(</span>
        <span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span>
        <span class="n">scale_diag</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">((</span><span class="n">x</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(),</span> <span class="n">y</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">prob</span> <span class="o">=</span> <span class="n">mvgmm</span><span class="p">.</span><span class="nf">prob</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">axes</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="sh">'</span><span class="s">3d</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">prob</span><span class="p">.</span><span class="nf">reshape</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)));</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot_surface</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">prob</span><span class="p">.</span><span class="nf">reshape</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">)),</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">viridis</span><span class="sh">'</span><span class="p">);</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/gmm-tfp/multivariate-gmm-pdf.png" data-zoomable=""/> </div> </div> <div class="caption"> Multivariate Gaussian mixture model probability density function. </div> <h2 id="conditional-multivariate-gaussian">Conditional Multivariate Gaussian</h2> <p>Unfortunately, TensorFlow-Probability does not provide support for obtaining the conditional and marginal distributions given the selected features of X. We can implement this ourselves by extending <code class="language-plaintext highlighter-rouge">tfd.MultivariateNormalTriL</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">invert_indices</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
    <span class="n">inv</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nb">bool</span><span class="p">)</span>
    <span class="n">inv</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">inv</span><span class="p">,</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">inv</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inv</span>
<span class="k">class</span> <span class="nc">ConditionalMultivariateNormal</span><span class="p">(</span><span class="n">tfd</span><span class="p">.</span><span class="n">MultivariateNormalTriL</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">covariances</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">covariance</span><span class="p">()</span>
        <span class="n">means</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">loc</span>
        <span class="k">return</span> <span class="n">means</span><span class="p">,</span> <span class="n">covariances</span>
    
    <span class="k">def</span> <span class="nf">condition</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">i2</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">cov</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">loc</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="nf">covariance</span><span class="p">()</span>
        <span class="n">i1</span> <span class="o">=</span> <span class="nf">invert_indices</span><span class="p">(</span><span class="n">mu</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">indices</span><span class="p">)</span>
        
        <span class="n">cov_12</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">cov</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">i2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">cov_11</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">cov</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">i1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">cov_22</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">cov</span><span class="p">,</span> <span class="n">i2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">i2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">prec_22</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">pinv</span><span class="p">(</span><span class="n">cov_22</span><span class="p">)</span>
        <span class="n">regression_coeffs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">tensordot</span><span class="p">(</span><span class="n">cov_12</span><span class="p">,</span> <span class="n">prec_22</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">mean</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">tf</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">i2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">mean</span> <span class="o">+=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">tensordot</span><span class="p">(</span><span class="n">regression_coeffs</span><span class="p">,</span> <span class="n">diff</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        
        <span class="n">covariance</span> <span class="o">=</span> <span class="n">cov_11</span> <span class="o">-</span> <span class="n">tf</span><span class="p">.</span><span class="nf">tensordot</span><span class="p">(</span><span class="n">regression_coeffs</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">cov_12</span><span class="p">),</span> <span class="n">axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="nc">ConditionalMultivariateNormal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale_tril</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">cholesky</span><span class="p">(</span><span class="n">covariance</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">marginalize</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">cov</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">loc</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="nf">covariance</span><span class="p">()</span>
        <span class="k">return</span> <span class="nc">ConditionalMultivariateNormal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()[</span><span class="n">indices</span><span class="p">],</span> <span class="n">scale_tril</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">cholesky</span><span class="p">(</span><span class="n">cov</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()[</span><span class="n">np</span><span class="p">.</span><span class="nf">ix_</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">indices</span><span class="p">)]))</span>
<span class="c1"># Conditional Distribution P(X1|X0)
</span><span class="n">mvn</span> <span class="o">=</span> <span class="nc">ConditionalMultivariateNormal</span><span class="p">(</span>
    <span class="n">loc</span><span class="o">=</span><span class="n">X_mean</span><span class="p">,</span>
    <span class="n">scale_tril</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">cholesky</span><span class="p">(</span><span class="n">X_cov</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<span class="n">conditional_mvn</span> <span class="o">=</span> <span class="n">mvn</span><span class="p">.</span><span class="nf">condition</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">marginal_mvn</span> <span class="o">=</span> <span class="n">mvn</span><span class="p">.</span><span class="nf">marginalize</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">conditional_mvn</span><span class="p">.</span><span class="nf">sample</span><span class="p">().</span><span class="nf">numpy</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="n">marginal_mvn</span><span class="p">.</span><span class="nf">sample</span><span class="p">().</span><span class="nf">numpy</span><span class="p">())</span>
<span class="c1"># Conditional MVN sample
</span><span class="p">[[[[</span><span class="mf">1.60346902</span><span class="p">]]]</span>
 <span class="p">[[[</span><span class="mf">0.70901248</span><span class="p">]]]</span>
 <span class="p">[[[</span><span class="mf">0.68173244</span><span class="p">]]]]</span>
<span class="c1"># Marginal MVN sample
</span><span class="p">[[</span><span class="o">-</span><span class="mf">0.22300554</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">2.69431439</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.52467359</span><span class="p">]]</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="tutorial"/><category term="data-science"/><summary type="html"><![CDATA[A note on implementing Gaussian mixture models with TensorFlow Probability.]]></summary></entry><entry><title type="html">Anomaly Detection of Time Series Data</title><link href="/blog/2019/anomaly-detection/" rel="alternate" type="text/html" title="Anomaly Detection of Time Series Data"/><published>2019-06-06T00:00:00+00:00</published><updated>2019-06-06T00:00:00+00:00</updated><id>/blog/2019/anomaly-detection</id><content type="html" xml:base="/blog/2019/anomaly-detection/"><![CDATA[<h2 id="overview">Overview</h2> <h3 id="definition---anomaly-detection">Definition - Anomaly Detection</h3> <p>Anomaly detection (also outlier detection) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. – Wikipedia</p> <h3 id="definition---anomaly">Definition - Anomaly</h3> <p>An anomaly is the deviation in a quantity from its expected value, e.g., the difference between a measurement and a mean or a model prediction. – Wikipedia</p> <h3 id="statistical-methods">Statistical Methods</h3> <ul> <li>Holt-Winters (Triple Exponential Smoothing)</li> <li>ARIMA (Auto-Regressive Integrated Moving Average)</li> <li>Histogram-Based Outlier Detection (HBOS)</li> </ul> <p>Conventional statistical methods are generally more interpretable and sometimes more useful than machine learning-based methods, depending on the specified problem.</p> <h3 id="machine-learning-methods">Machine Learning Methods</h3> <ul> <li>Supervised (e.g. Decision Tree, SVM, LSTM Forecasting)</li> <li>Unsupervised (e.g. K-Means, Hierarchical Clustering, DBSCAN)</li> <li>Self-Supervised (e.g. LSTM Autoencoder)</li> </ul> <p>Machine learning methods can model more complex data and hence able to detect more complex anomalies than conventional statistical methods.</p> <h3 id="data-representation">Data Representation</h3> <ul> <li>Point</li> <li>Rolling Window (or trajectory matrix)</li> <li>Time Series Features (transformations, decompositions and statistical measurements)</li> </ul> <h3 id="other-techniques">Other Techniques</h3> <ul> <li>Synthetic Anomaly Generation (e.g. GANs)</li> <li>Note-Worthy Libraries (tsfresh, fbprophet)</li> </ul> <h2 id="statistical-methods-1">Statistical Methods</h2> <h3 id="holt-winters-triple-exponential-smoothing">Holt-Winters (Triple Exponential Smoothing)</h3> <p>Holt-Winters is a forecasting technique for seasonal (i.e. cyclical) time series data, based on previous timestamps.</p> <p>Holt-Winters models a time series in 3 ways – average, trend and seasonality. An average is a value referenced upon, a trend is a general increase/decrease over time and a seasonality is a cyclical repeating pattern over a period.</p> <p>Equation: ŷ x = α⋅yx + (1−α)⋅ŷ x−1</p> <p>The value forecast at t=x is a factor of the value at t=x, along with a discounted value of the value forecast at t=x-1. (1-a) is recursively multiplied every timestamp back, resulting in an exponential computation.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from statsmodels.tsa.holtwinters import ExponentialSmoothing

fit = ExponentialSmoothing(data, seasonal_periods=periodicity, trend='add', seasonal='add').fit(use_boxcox=True)
fit.fittedvalues.plot(color='blue')
fit.forecast(5).plot(color='green')
plt.show()
</code></pre></div></div> <h3 id="arima-auto-regressive-integrated-moving-average">ARIMA (Auto-Regressive Integrated Moving Average)</h3> <p>ARIMA is a statistical model for time series data, capturing 3 key aspects of the temporal information — Auto-Regression(AR), Integration(I) and Moving Average(MA).</p> <ul> <li>Auto-Regression — Observations are regressed on its own lagged (i.e., prior) values.</li> <li>Integrated — Data values are replaced by the difference between values.</li> <li>Moving Average — Regression errors are dependent on lagged observations.</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from statsmodels.tsa.arima_model import ARIMA
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

p = 5  # lag
d = 1  # difference order
q = 0  # size of moving average window

train, test = train_test_split(X, test_size=0.20, shuffle=False)
history = train.tolist()
predictions = []

for t in range(len(test)):
	model = ARIMA(history, order=(p,d,q))
	fit = model.fit(disp=False)
	pred = fit.forecast()[0]
  
	predictions.append(pred)
	history.append(test[t])
  
print('MSE: %.3f' % mean_squared_error(test, predictions))

plt.plot(test)
plt.plot(predictions, color='red')
plt.show()
</code></pre></div></div> <h3 id="histogram-based-outlier-detection">Histogram-Based Outlier Detection</h3> <p>Histogram-Based Outlier Score (HBOS) is a O(n) linear time unsupervised algorithm that is faster than multivariate approaches at the cost of less precision. It can detect global outliers well but performs poorly on local outlier problems.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from kenchi.outlier_detection.statistical import HBOS

hbos = HBOS(novelty=True).fit(X)
y_pred = hbos.predict(X)
</code></pre></div></div> <h2 id="machine-learning-methods-1">Machine Learning Methods</h2> <h3 id="decision-tree--supervised">Decision Tree — Supervised</h3> <p>Decision Trees are used for anomaly detection to learn rules from data. However, with few labelled data, aside from the class imbalance problem, inferences from rules may not make sense, as leaf nodes may end up with very few observations, e.g. 2 positives and 0 negatives.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=True)

clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

print(classification_report(y_test, y_pred))
</code></pre></div></div> <h3 id="support-vector-machines-svm--supervised">Support Vector Machines (SVM) — Supervised</h3> <p>SVMs first maps input vectors into a higher-dimensional feature space, then obtains the optimal separating hyper-plane in the feature space. The decision boundary is determined by support vectors rather than the whole training sample, and thus is extremely robust to outliers.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=True)

clf = SVC(gamma='auto')
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

print(classification_report(y_test, y_pred))
</code></pre></div></div> <h3 id="lstm-forecasting--supervised">LSTM Forecasting — Supervised</h3> <p>LSTM Forecasting is a supervised method that, given a time series sequence as input, predicts the value at the next timestamp. It trains on normal data only, and the prediction error is used as the anomaly score.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from keras.models import Sequential
from keras.layers import LSTM, Dense
from sklearn.metrics import mean_squared_error

timesteps = window_size-1
n_features = 1

model = Sequential()
model.add(LSTM(16, activation='relu', input_shape=(timesteps, n_features), return_sequences=True))
model.add(LSTM(16, activation='relu'))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')

model.fit(X_train, y_train, epochs=30, batch_size=32)
y_pred = model.predict(X_test)
print("MSE:", mean_squared_error(y_test, y_pred))
</code></pre></div></div> <h3 id="k-means-clustering--unsupervised">K-Means Clustering — Unsupervised</h3> <p>K-Means Clustering is generally not useful in anomaly detection due to its sensitivity to outliers. Centroids cannot be updated if a set of objects close to it is empty.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.cluster import KMeans

clusters = 3
y_pred = KMeans(n_clusters=clusters).fit_predict(X)

plt.scatter(X[:,0], X[:,1], c=y_pred)
plt.show()
</code></pre></div></div> <h3 id="hierarchical-clustering--unsupervised">Hierarchical Clustering — Unsupervised</h3> <p>Hierarchical Clustering, unlike K-Means, does not require specification of the number of clusters at initialisation. It creates a dendrogram and clusters can be separately selected thereafter. Scipy’s Hierarchical Clustering is recommended over Scikit-Learn’s because of its customisability, ability to select number of clusters after the clustering, and dendrogram plots.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
from sklearn.cluster import AgglomerativeClustering

clusters = 3
y_pred = AgglomerativeClustering(n_clusters=clusters).fit_predict(X)


from scipy.cluster.hierarchy import linkage, fcluster, dendrogram

clusters=5
cls = linkage(X, method='ward')
y_pred = fcluster(cls, t=clusters, criterion='maxclust')

dendrogram(cls)
plt.show()
</code></pre></div></div> <h3 id="lstm-autoencoder--self-supervised">LSTM Autoencoder — Self-Supervised</h3> <p>LSTM Autoencoder is a self-supervised method that, given a time series sequence as input, predicts the same input sequence as its output. With this approach, it learns a representation of normal sequences and the prediction error can be interpreted as the anomaly score.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed
from keras.models import Sequential

class LSTM_Autoencoder:
  def __init__(self, optimizer='adam', loss='mse'):
    self.optimizer = optimizer
    self.loss = loss
    self.n_features = 1
    
  def build_model(self):
    timesteps = self.timesteps
    n_features = self.n_features
    model = Sequential()
    
    # Encoder
    model.add(LSTM(timesteps, activation='relu', input_shape=(timesteps, n_features), return_sequences=True))
    model.add(LSTM(16, activation='relu', return_sequences=True))
    model.add(LSTM(1, activation='relu'))
    model.add(RepeatVector(timesteps))
    
    # Decoder
    model.add(LSTM(timesteps, activation='relu', return_sequences=True))
    model.add(LSTM(16, activation='relu', return_sequences=True))
    model.add(TimeDistributed(Dense(n_features)))
    
    model.compile(optimizer=self.optimizer, loss=self.loss)
    model.summary()
    self.model = model
    
  def fit(self, X, epochs=3, batch_size=32):
    self.timesteps = X.shape[1]
    self.build_model()
    
    input_X = np.expand_dims(X, axis=2)
    self.model.fit(input_X, input_X, epochs=epochs, batch_size=batch_size)
    
  def predict(self, X):
    input_X = np.expand_dims(X, axis=2)
    output_X = self.model.predict(input_X)
    reconstruction = np.squeeze(output_X)
    return np.linalg.norm(X - reconstruction, axis=-1)
  
  def plot(self, scores, timeseries, threshold=0.95):
    sorted_scores = sorted(scores)
    threshold_score = sorted_scores[round(len(scores) * threshold)]
    
    plt.title("Reconstruction Error")
    plt.plot(scores)
    plt.plot([threshold_score]*len(scores), c='r')
    plt.show()
    
    anomalous = np.where(scores &gt; threshold_score)
    normal = np.where(scores &lt;= threshold_score)
    
    plt.title("Anomalies")
    plt.scatter(normal, timeseries[normal][:,-1], s=3)
    plt.scatter(anomalous, timeseries[anomalous][:,-1], s=5, c='r')
    plt.show()
    
lstm_autoencoder = LSTM_Autoencoder(optimizer='adam', loss='mse')
lstm_autoencoder.fit(normal_timeseries, epochs=3, batch_size=32)
scores = lstm_autoencoder.predict(test_timeseries)
lstm_autoencoder.plot(scores, test_timeseries, threshold=0.95)
</code></pre></div></div> <h3 id="data-representation-1">Data Representation</h3> <ul> <li>Point <ul> <li>Given a only a point, contextual anomalies cannot be detected due to the lack of temporal information.</li> </ul> </li> <li>Rolling Window <ul> <li>A rolling window (representing a point) contains temporal information from a few time steps back, allowing the possibility of detecting contextual anomalies. This is sufficient for LSTM-based models. ``` from skimage.util import view_as_windows</li> </ul> <p>window_size = 5 timeseries = np.array([1,2,3,4,5,6,7,8,9,10])</p> <p>trajectory_matrix = view_as_windows(timeseries, window_shape=window_size) ```</p> </li> <li>Time Series Features <ul> <li>Signal transformations/decompositions such as Fast Fourier Transform (FFT), Continuous Wavelet Transform (CWT) and Singular Spectrum Analysis (SSA), as well as statistical measurements such as Max/Min, Mean, No. of Peaks, can surface temporal information crucial to detecting anomalies.</li> </ul> </li> </ul> <h3 id="other-techniques-1">Other Techniques</h3> <p>Synthetic Anomaly Generation</p> <ul> <li>Importance <ul> <li>Synthetic anomaly generation is important due to the usual case of lack of labelled anomalous data. Without a good set of known anomalies (in variety and volume), evaluation of anomaly detection models cannot be reliable.</li> </ul> </li> <li>Rule-based Generator <ul> <li>Based on domain expertise, rule-based generation of anomalous data can cover the scope/variety of most possible types of anomalies.</li> </ul> </li> <li>Generative Adversarial Network <ul> <li>Given few data, GANs can generate anomalous data, albeit difficult, due to mode collapse, training instability and noisy generated signals.</li> </ul> </li> </ul> <h3 id="note-worthy-libraries">Note-Worthy Libraries</h3> <ul> <li>tsfresh - Automatic Time Series Feature Extraction (<a href="https://github.com/blue-yonder/tsfresh">Open-source</a>)</li> <li>fbprophet - Facebook’s Time Series Forecasting Model (<a href="https://facebook.github.io/prophet/">Open-source</a>)</li> </ul> <h2 id="references">References</h2> <ol> <li><a href="https://www.statsmodels.org/dev/examples/notebooks/generated/exponential_smoothing.html">Exponential smoothing — StatsModels</a></li> <li><a href="https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/">How to Create an ARIMA Model for Time Series Forecasting in Python</a></li> <li><a href="https://www.researchgate.net/publication/231614824_Histogram-based_Outlier_Score_HBOS_A_fast_Unsupervised_Anomaly_Detection_Algorithm">Histogram-based Outlier Score (HBOS): A fast Unsupervised Anomaly Detection Algorithm</a></li> <li><a href="https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/">How to Develop LSTM Models for Time Series Forecasting</a></li> <li><a href="https://arxiv.org/abs/1607.00148">LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection</a></li> </ol>]]></content><author><name></name></author><category term="tutorial"/><category term="data-science"/><summary type="html"><![CDATA[A note on anomaly detection techniques, evaluation and applications on time series data.]]></summary></entry></feed>